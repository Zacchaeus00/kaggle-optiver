{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from tqdm import tqdm \n",
    "from glob import glob\n",
    "import joblib\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import copy as cp\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '../input/optiver-realized-volatility-prediction'\n",
    "model_dir = '../input/quant-model'\n",
    "stock_encode_dir = '../input/quant-stock-encode'\n",
    "features_name_dir = '../input/quant-features-name'\n",
    "features_name_dir = '../data'\n",
    "stock_encode_dir = '../data'\n",
    "model_dir = '../model'\n",
    "input_dir = '../data'\n",
    "# input_dir = '../data'\n",
    "# model_dir = '../model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_book(stock_id=0, data_type='train'):\n",
    "    \"\"\"加载 book 数据\n",
    "    \"\"\"\n",
    "    book_df = pd.read_parquet(\n",
    "        os.path.join(input_dir,\n",
    "                     'book_{}.parquet/stock_id={}'.format(data_type,\n",
    "                                                          stock_id)))\n",
    "    book_df['stock_id'] = stock_id\n",
    "    book_df['stock_id'] = book_df['stock_id'].astype(np.int8)\n",
    "    book_df['seconds_in_bucket'] = book_df['seconds_in_bucket'].astype(\n",
    "        np.int32)\n",
    "\n",
    "    return book_df\n",
    "\n",
    "\n",
    "def load_trade(stock_id=0, data_type='train'):\n",
    "    \"\"\"加载 trade 数据\n",
    "    \"\"\"\n",
    "    trade_df = pd.read_parquet(\n",
    "        os.path.join(\n",
    "            input_dir,\n",
    "            'trade_{}.parquet/stock_id={}'.format(data_type, stock_id)))\n",
    "    trade_df['stock_id'] = stock_id\n",
    "    trade_df['stock_id'] = trade_df['stock_id'].astype(np.int8)\n",
    "    trade_df['order_count'] = trade_df['order_count'].astype(np.int32)\n",
    "    trade_df['seconds_in_bucket'] = trade_df['seconds_in_bucket'].astype(\n",
    "        np.int32)\n",
    "\n",
    "    return trade_df\n",
    "def log_return(list_stock_prices):\n",
    "    \"\"\"收益率\n",
    "    \"\"\"\n",
    "    return np.log(list_stock_prices).diff()\n",
    "\n",
    "\n",
    "def realized_volatility(series_log_return):\n",
    "    \"\"\"波动率\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.sum(series_log_return**2))\n",
    "\n",
    "\n",
    "def fix_jsonerr(df):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    df.columns = [\n",
    "        \"\".join(c if c.isalnum() else \"_\" for c in str(x)) for x in df.columns\n",
    "    ]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征工程\n",
    "def feature_row(book):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # book_wap1 生成标签\n",
    "    for i in [\n",
    "            1,\n",
    "            2,\n",
    "    ]:\n",
    "        # wap\n",
    "        book[f'book_wap{i}'] = (book[f'bid_price{i}'] * book[f'ask_size{i}'] +\n",
    "                                book[f'ask_price{i}'] *\n",
    "                                book[f'bid_size{i}']) / (book[f'bid_size{i}'] +\n",
    "                                                         book[f'ask_size{i}'])\n",
    "\n",
    "    # mean wap\n",
    "    book['book_wap_mean'] = (book['book_wap1'] + book['book_wap2']) / 2\n",
    "\n",
    "    # wap diff\n",
    "    book['book_wap_diff'] = book['book_wap1'] - book['book_wap2']\n",
    "\n",
    "    # other orderbook features\n",
    "    book['book_price_spread'] = (book['ask_price1'] - book['bid_price1']) / (\n",
    "        book['ask_price1'] + book['bid_price1'])\n",
    "    book['book_bid_spread'] = book['bid_price1'] - book['bid_price2']\n",
    "    book['book_ask_spread'] = book['ask_price1'] - book['ask_price2']\n",
    "    book['book_total_volume'] = book['ask_size1'] + book['ask_size2'] + book[\n",
    "        'bid_size1'] + book['bid_size2']\n",
    "    book['book_volume_imbalance'] = (book['ask_size1'] + book['ask_size2']) - (\n",
    "        book['bid_size1'] + book['bid_size2'])\n",
    "    return book\n",
    "\n",
    "\n",
    "def feature_agg(book, trade):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # 聚合生成特征\n",
    "    book_feats = book.columns[book.columns.str.startswith('book_')].tolist()\n",
    "    trade_feats = ['price', 'size', 'order_count', 'seconds_in_bucket']\n",
    "\n",
    "    trade = trade.groupby(['time_id', 'stock_id'])[trade_feats].agg(\n",
    "        ['sum', 'mean', 'std', 'max', 'min']).reset_index()\n",
    "\n",
    "    book = book.groupby(['time_id', 'stock_id'])[book_feats].agg(\n",
    "        [lambda x: realized_volatility(log_return(x))]).reset_index()\n",
    "\n",
    "    # 修改特征名称\n",
    "    book.columns = [\"\".join(col).strip() for col in book.columns.values]\n",
    "    trade.columns = [\"\".join(col).strip() for col in trade.columns.values]\n",
    "    df_ret = book.merge(trade, how='left', on=['time_id', 'stock_id'])\n",
    "    return df_ret\n",
    "\n",
    "\n",
    "def gen_data_train(stock_id=0):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    book = load_book(stock_id, 'train')\n",
    "    trade = load_trade(stock_id, 'train')\n",
    "\n",
    "    book = book.sort_values(by=['time_id', 'seconds_in_bucket'])\n",
    "    trade = trade.sort_values(by=['time_id', 'seconds_in_bucket'])\n",
    "\n",
    "    book = feature_row(book)\n",
    "\n",
    "    df_ret1 = feature_agg(book, trade)\n",
    "\n",
    "    df_ret2 = calculate_features2(book, trade)\n",
    "\n",
    "    return df_ret2.merge(df_ret1, how='left', on='time_id')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def gen_data_test(stock_id=0):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    book = load_book(stock_id, 'test')\n",
    "    trade = load_trade(stock_id, 'test')\n",
    "\n",
    "    book = book.sort_values(by=['time_id', 'seconds_in_bucket'])\n",
    "    trade = trade.sort_values(by=['time_id', 'seconds_in_bucket'])\n",
    "\n",
    "    book = feature_row(book)\n",
    "\n",
    "    df_ret1 = feature_agg(book, trade)\n",
    "    print(df_ret1)\n",
    "    df_ret2 = calculate_features2(book, trade)\n",
    "    print(df_ret2)\n",
    "    return df_ret2.merge(df_ret1, how='left', on='time_id')\n",
    "\n",
    "\n",
    "def gen_data_multi(stock_lst, data_type='train'):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    with Pool(cpu_count()) as p:\n",
    "        if data_type == 'train':\n",
    "            feature_dfs = list(\n",
    "                tqdm(p.imap(gen_data_train, stock_lst), total=len(stock_lst)))\n",
    "        if data_type == 'test':\n",
    "            feature_dfs = list(\n",
    "                tqdm(p.imap(gen_data_test, stock_lst), total=len(stock_lst)))\n",
    "    print(feature_dfs)\n",
    "    df_ret = pd.concat(feature_dfs)\n",
    "    return df_ret\n",
    "\n",
    "\n",
    "def gen_data_encoding(df_ret, df_label, data_type='train'):\n",
    "    \"\"\"\n",
    "    test 不使用自己数据的 stock_id encoding\n",
    "    \"\"\"\n",
    "\n",
    "    # 对 stock_id 进行 encoding\n",
    "    vol_feats = [f for f in df_ret.columns if ('lambda' in f) & ('wap' in f)]\n",
    "    if data_type == 'train':\n",
    "        # agg\n",
    "        stock_df = df_ret.groupby('stock_id')[vol_feats].agg([\n",
    "            'mean',\n",
    "            'std',\n",
    "            'max',\n",
    "            'min',\n",
    "        ]).reset_index()\n",
    "\n",
    "        # fix column names\n",
    "        stock_df.columns = ['stock_id'] + [\n",
    "            f'{f}_stock' for f in stock_df.columns.values.tolist()[1:]\n",
    "        ]\n",
    "        stock_df = fix_jsonerr(stock_df)\n",
    "\n",
    "    # 对 time_id 进行 encoding\n",
    "    time_df = df_ret.groupby('time_id')[vol_feats].agg([\n",
    "        'mean',\n",
    "        'std',\n",
    "        'max',\n",
    "        'min',\n",
    "    ]).reset_index()\n",
    "    time_df.columns = ['time_id'] + [\n",
    "        f'{f}_time' for f in time_df.columns.values.tolist()[1:]\n",
    "    ]\n",
    "\n",
    "    # merge\n",
    "    df_ret = df_ret.merge(time_df, how='left', on='time_id')\n",
    "\n",
    "    # make sure to fix json error for lighgbm\n",
    "    df_ret = fix_jsonerr(df_ret)\n",
    "\n",
    "    # out\n",
    "    if data_type == 'train':\n",
    "        df_ret = df_ret.merge(stock_df, how='left', on='stock_id').merge(\n",
    "            df_label, how='left',\n",
    "            on=['stock_id', 'time_id']).replace([np.inf, -np.inf],\n",
    "                                                np.nan).fillna(method='ffill')\n",
    "        return df_ret\n",
    "    if data_type == 'test':\n",
    "        stock_df = pd.read_pickle(os.path.join(stock_encode_dir,'20210809.pkl'))\n",
    "        df_ret = df_ret.merge(stock_df, how='left', on='stock_id').replace(\n",
    "            [np.inf, -np.inf], np.nan).fillna(method='ffill')\n",
    "        return df_ret\n",
    "\n",
    "\n",
    "def calc_rollingstats(rolling_x, roll_name):\n",
    "    #统计量\n",
    "    if len(rolling_x) > 0:\n",
    "        roll_autocorr = rolling_x.groupby(\"time_id\")[[roll_name,\n",
    "                                                      \"xpre\"]].corr()\n",
    "        roll_autocorr.reset_index(inplace=True)\n",
    "        roll_autocorr = roll_autocorr.groupby(\"time_id\").head(1)\n",
    "        roll_autocorr.index = roll_autocorr[\"time_id\"]\n",
    "        del roll_autocorr[\"time_id\"]\n",
    "\n",
    "        roll_autocorr = pd.DataFrame(\n",
    "            {roll_name + \"_autocorr\": roll_autocorr[\"xpre\"]})\n",
    "\n",
    "        roll_mean = pd.DataFrame({\n",
    "            roll_name + \"_mean\":\n",
    "            rolling_x.groupby(\"time_id\")[roll_name].mean()\n",
    "        })\n",
    "        roll_std = pd.DataFrame({\n",
    "            roll_name + \"_std\":\n",
    "            rolling_x.groupby(\"time_id\")[roll_name].std()\n",
    "        })\n",
    "        roll_skew = pd.DataFrame({\n",
    "            roll_name + \"_skew\":\n",
    "            rolling_x.groupby(\"time_id\")[roll_name].skew()\n",
    "        })\n",
    "\n",
    "        data_merge = pd.merge(roll_mean,\n",
    "                              roll_std,\n",
    "                              left_index=True,\n",
    "                              right_index=True,\n",
    "                              how=\"inner\")\n",
    "        data_merge = pd.merge(data_merge,\n",
    "                              roll_skew,\n",
    "                              left_index=True,\n",
    "                              right_index=True,\n",
    "                              how=\"inner\")\n",
    "        data_merge = pd.merge(data_merge,\n",
    "                              roll_autocorr,\n",
    "                              left_index=True,\n",
    "                              right_index=True,\n",
    "                              how=\"inner\")\n",
    "\n",
    "    else:\n",
    "\n",
    "        data_merge = pd.DataFrame([[np.nan, np.nan, np.nan, np.nan]])\n",
    "        data_merge.columns = [\n",
    "            roll_name + \"_mean\", roll_name + \"_std\", roll_name + \"_skew\",\n",
    "            roll_name + \"_autocorr\"\n",
    "        ]\n",
    "\n",
    "    return data_merge\n",
    "\n",
    "\n",
    "\n",
    "def make_candle(df_data, price_name, vol_name, amt_name):\n",
    "\n",
    "    df_data[\"pre\"] = df_data.groupby(\"time_id\")[price_name].shift(1)\n",
    "    df_data[\"ret\"] = df_data[price_name] / df_data[\"pre\"] - 1\n",
    "    df_data[\"absret\"] = abs(df_data[\"ret\"])\n",
    "    df_retsum = pd.DataFrame(\n",
    "        {\"retsum\": df_data.groupby(\"time_id\")[\"ret\"].sum()})\n",
    "    df_absretsum = pd.DataFrame(\n",
    "        {\"absretsum\": df_data.groupby(\"time_id\")[\"absret\"].sum()})\n",
    "\n",
    "    df_data[\"absobv\"] = df_data[\"absret\"] * df_data[vol_name]\n",
    "    df_obvabs = pd.DataFrame(\n",
    "        {\"xf4_abs\": df_data.groupby(\"time_id\")[\"absobv\"].sum()})\n",
    "\n",
    "    df_data[\"obv\"] = df_data[\"ret\"] * df_data[vol_name]\n",
    "    df_obv = pd.DataFrame({\"xf4\": df_data.groupby(\"time_id\")[\"obv\"].sum()})\n",
    "\n",
    "    df_amt = pd.DataFrame(\n",
    "        {amt_name + \"sum\": df_data.groupby(\"time_id\")[amt_name].sum()})\n",
    "    df_vol = pd.DataFrame(\n",
    "        {vol_name + \"sum\": df_data.groupby(\"time_id\")[vol_name].sum()})\n",
    "\n",
    "    df_mean = pd.DataFrame(\n",
    "        {price_name + \"mean\": df_data.groupby(\"time_id\")[price_name].mean()})\n",
    "    df_high = pd.DataFrame(\n",
    "        {price_name + \"high\": df_data.groupby(\"time_id\")[price_name].max()})\n",
    "    df_low = pd.DataFrame(\n",
    "        {price_name + \"low\": df_data.groupby(\"time_id\")[price_name].min()})\n",
    "\n",
    "    df_open = df_data.groupby(\"time_id\").head(1)\n",
    "    df_open.index = df_open[\"time_id\"]\n",
    "    df_open = pd.DataFrame({price_name + \"open\": df_open[price_name]})\n",
    "\n",
    "    df_close = df_data.groupby(\"time_id\").tail(1)\n",
    "    df_close.index = df_close[\"time_id\"]\n",
    "    df_close = pd.DataFrame({price_name + \"close\": df_close[price_name]})\n",
    "\n",
    "    df_candle = pd.merge(df_high,\n",
    "                         df_low,\n",
    "                         left_index=True,\n",
    "                         right_index=True,\n",
    "                         how=\"inner\")\n",
    "    df_candle = pd.merge(df_candle,\n",
    "                         df_mean,\n",
    "                         left_index=True,\n",
    "                         right_index=True,\n",
    "                         how=\"inner\")\n",
    "    df_candle = pd.merge(df_candle,\n",
    "                         df_open,\n",
    "                         left_index=True,\n",
    "                         right_index=True,\n",
    "                         how=\"inner\")\n",
    "    df_candle = pd.merge(df_candle,\n",
    "                         df_close,\n",
    "                         left_index=True,\n",
    "                         right_index=True,\n",
    "                         how=\"inner\")\n",
    "    df_candle = pd.merge(df_candle,\n",
    "                         df_vol,\n",
    "                         left_index=True,\n",
    "                         right_index=True,\n",
    "                         how=\"inner\")\n",
    "    df_candle = pd.merge(df_candle,\n",
    "                         df_amt,\n",
    "                         left_index=True,\n",
    "                         right_index=True,\n",
    "                         how=\"inner\")\n",
    "    df_candle = pd.merge(df_candle,\n",
    "                         df_retsum,\n",
    "                         left_index=True,\n",
    "                         right_index=True,\n",
    "                         how=\"inner\")\n",
    "    df_candle = pd.merge(df_candle,\n",
    "                         df_absretsum,\n",
    "                         left_index=True,\n",
    "                         right_index=True,\n",
    "                         how=\"inner\")\n",
    "    df_candle = pd.merge(df_candle,\n",
    "                         df_obvabs,\n",
    "                         left_index=True,\n",
    "                         right_index=True,\n",
    "                         how=\"inner\")\n",
    "    df_candle = pd.merge(df_candle,\n",
    "                         df_obv,\n",
    "                         left_index=True,\n",
    "                         right_index=True,\n",
    "                         how=\"inner\")\n",
    "\n",
    "    return df_candle\n",
    "\n",
    "\n",
    "def cal_candlefactor(df_candle, price_name, vol_name, amt_name):\n",
    "    f_name = price_name + \"candle\"\n",
    "    #f1:illiq\n",
    "    df_candle[f_name + \"f1\"] = (\n",
    "        2 * (df_candle[price_name + \"high\"] - df_candle[price_name + \"low\"]) -\n",
    "        abs(df_candle[price_name + \"open\"] -\n",
    "            df_candle[price_name + \"close\"])) / df_candle[amt_name + \"sum\"]\n",
    "    #f2 strength\n",
    "    df_candle[f_name + \"f2\"] = df_candle[\"retsum\"] / df_candle[\"absretsum\"]\n",
    "    #f3:ad\n",
    "    df_candle[f_name + \"f3\"] =  (2 *df_candle[price_name + \"close\"] - df_candle[price_name + \"low\"]\\\n",
    "                    - df_candle[price_name + \"high\"] )/(df_candle[price_name + \"high\"] - df_candle[price_name + \"low\"]) \\\n",
    "                    * df_candle[vol_name + \"sum\"]\n",
    "    #f3: obv\n",
    "    df_candle[f_name + \"f41\"] = df_candle[\"xf4\"] / df_candle[vol_name + \"sum\"]\n",
    "    df_candle[f_name +\n",
    "              \"f42\"] = df_candle[\"xf4_abs\"] / df_candle[vol_name + \"sum\"]\n",
    "    return df_candle\n",
    "\n",
    "\n",
    "def calculate_features2(book_df, trade_df):\n",
    "    \"\"\"\n",
    "    df: book_train data for each stock_id\n",
    "    \"\"\"\n",
    "    #calculate price for features\n",
    "\n",
    "    book_df['wap'] = (book_df['bid_price1'] * book_df['ask_size1'] +\n",
    "                      book_df['ask_price1'] * book_df['bid_size1']) / (\n",
    "                          book_df['bid_size1'] + book_df['ask_size1'])\n",
    "\n",
    "    book_df[\"vol_ab\"] = book_df['bid_size1'] + book_df['ask_size1']\n",
    "    book_df[\"amt_ab\"] = book_df['bid_price1'] * book_df['ask_size1'] + book_df[\n",
    "        'ask_price1'] * book_df['bid_size1']\n",
    "\n",
    "    book_df[\"amt_a\"] = book_df['ask_price1'] * book_df['ask_size1']\n",
    "    book_df[\"amt_b\"] = book_df['bid_price1'] * book_df['bid_size1']\n",
    "\n",
    "    trade_df[\"amt\"] = trade_df[\"price\"] * trade_df[\"size\"]\n",
    "\n",
    "    #flag filter\n",
    "    book_df[\"wap_pre\"] = book_df.groupby(\"time_id\")['wap'].shift(1)\n",
    "    book_df[\"bid_ppre\"] = book_df.groupby(\"time_id\")['bid_price1'].shift(1)\n",
    "    book_df[\"ask_ppre\"] = book_df.groupby(\"time_id\")['ask_price1'].shift(1)\n",
    "\n",
    "    book_df[\"isBS\"] = np.where(\n",
    "        book_df[\"wap\"] > book_df[\"wap_pre\"], \"B\",\n",
    "        np.where(book_df[\"wap\"] < book_df[\"wap_pre\"], \"S\", np.nan))\n",
    "    book_df[\"isBS_big\"] = np.where(\n",
    "        book_df[\"wap\"] > book_df[\"ask_ppre\"], \"supB\",\n",
    "        np.where(book_df[\"wap\"] < book_df[\"bid_ppre\"], \"supS\",\n",
    "                 np.where(pd.notnull(book_df[\"wap\"]), \"midBS\", np.nan)))\n",
    "\n",
    "    ordersize50 = pd.DataFrame({\n",
    "        \"ordersize50\":\n",
    "        book_df.groupby(\"time_id\")[\"amt_ab\"].apply(lambda x: np.nanmedian(x))\n",
    "    })\n",
    "    ordersize50.reset_index(inplace=True)\n",
    "\n",
    "    ordersize25 = pd.DataFrame({\n",
    "        \"ordersize25\":\n",
    "        book_df.groupby(\"time_id\")[\"amt_ab\"].apply(\n",
    "            lambda x: np.nanpercentile(x, 75))\n",
    "    })\n",
    "    ordersize25.reset_index(inplace=True)\n",
    "\n",
    "    ordersize75 = pd.DataFrame({\n",
    "        \"ordersize75\":\n",
    "        book_df.groupby(\"time_id\")[\"amt_ab\"].apply(\n",
    "            lambda x: np.nanpercentile(x, 25))\n",
    "    })\n",
    "    ordersize75.reset_index(inplace=True)\n",
    "    book_df1 = pd.merge(book_df, ordersize50, on=\"time_id\", how=\"left\")\n",
    "    book_df1 = pd.merge(book_df1, ordersize25, on=\"time_id\", how=\"left\")\n",
    "    book_df1 = pd.merge(book_df1, ordersize75, on=\"time_id\", how=\"left\")\n",
    "    book_df1.loc[:, \"isoversize50\"] = np.where(\n",
    "        book_df1[\"amt_ab\"] > book_df1[\"ordersize50\"], \"up50\",\n",
    "        np.where(book_df1[\"amt_ab\"] <= book_df1[\"ordersize50\"], \"down50\",\n",
    "                 np.nan))\n",
    "\n",
    "    book_df1.loc[:, \"isoversize75\"] = np.where(\n",
    "        book_df1[\"amt_ab\"] > book_df1[\"ordersize75\"], \"up75\",\n",
    "        np.where(book_df1[\"amt_ab\"] <= book_df1[\"ordersize75\"], \"down75\",\n",
    "                 np.nan))\n",
    "    book_df1.loc[:, \"isoversize25\"] = np.where(\n",
    "        book_df1[\"amt_ab\"] > book_df1[\"ordersize25\"], \"up25\",\n",
    "        np.where(book_df1[\"amt_ab\"] <= book_df1[\"ordersize25\"], \"down25\",\n",
    "                 np.nan))\n",
    "\n",
    "    #不同波动率\n",
    "    #calculate historical volatility\n",
    "    vol = book_df1.groupby('time_id')['wap'].apply(\n",
    "        lambda x: np.sqrt(np.sum(np.log(x).diff()**2)))\n",
    "    vol_df = pd.DataFrame(vol)\n",
    "    vol_df.rename(columns={'wap': 'vol_orig'}, inplace=True)\n",
    "    data_merge_all = vol_df\n",
    "\n",
    "    #修改波动率：\n",
    "    #    rolling波动率均值，标准差，偏度，自相关\n",
    "    #新指标,新指标均值，标准差，偏度，自相关\n",
    "\n",
    "    #    roll_name0 = \"roll_std\"\n",
    "    #    roll_window = 10\n",
    "    #BS FLAG\n",
    "    flagname = \"B\"\n",
    "    filtername = \"isBS\"\n",
    "    for filtername, flagname in [[\"isBS\", \"B\"], [\"isBS\", \"S\"],\n",
    "                                 [\"isBS_big\", \"supB\"], [\"isBS_big\", \"supS\"],\n",
    "                                 [\"isBS_big\", \"midBS\"],\n",
    "                                 [\"isoversize50\", \"up50\"],\n",
    "                                 [\"isoversize50\", \"down50\"],\n",
    "                                 [\"isoversize25\", \"up25\"],\n",
    "                                 [\"isoversize25\", \"down25\"],\n",
    "                                 [\"isoversize75\", \"up75\"],\n",
    "                                 [\"isoversize75\", \"down75\"]]:\n",
    "        print(filtername, flagname)\n",
    "        book_df_new = book_df1[book_df1[filtername] == flagname]\n",
    "        #个数\n",
    "        df_fnum = pd.DataFrame({\n",
    "            flagname + \"num\":\n",
    "            book_df_new.groupby(\"time_id\")[\"seconds_in_bucket\"].count()\n",
    "        })\n",
    "        data_merge_all = pd.merge(data_merge_all,\n",
    "                                  df_fnum,\n",
    "                                  left_index=True,\n",
    "                                  right_index=True,\n",
    "                                  how=\"left\")\n",
    "\n",
    "        for roll_window in [5, 10]:\n",
    "            #rolling指标\n",
    "            price_name = \"wap\"\n",
    "            roll_name0 = price_name + \"roll_std\"\n",
    "            roll_name = roll_name0 + str(roll_window) + \"_\" + flagname\n",
    "\n",
    "            rolling_x = pd.DataFrame({\n",
    "                roll_name:\n",
    "                book_df_new.groupby(\"time_id\")[price_name].rolling(\n",
    "                    roll_window).std()\n",
    "            })\n",
    "            rolling_x.reset_index(inplace=True)\n",
    "            rolling_x.loc[:, \"xpre\"] = rolling_x.groupby(\n",
    "                \"time_id\")[roll_name].shift(1)\n",
    "            #计算统计量因子\n",
    "            data_merge = calc_rollingstats(rolling_x, roll_name)\n",
    "            data_merge_all = pd.merge(data_merge_all,\n",
    "                                      data_merge,\n",
    "                                      left_index=True,\n",
    "                                      right_index=True,\n",
    "                                      how=\"left\")\n",
    "\n",
    "        #全局做candle：wap，买盘，卖盘\n",
    "        #candle因子\n",
    "        price_name = \"wap\"\n",
    "        vol_name = \"vol_ab\"\n",
    "        amt_name = \"amt_ab\"\n",
    "        df_data = cp.deepcopy(book_df_new)\n",
    "        df_candle = make_candle(df_data, price_name, vol_name, amt_name)\n",
    "\n",
    "        list_save = [\n",
    "            price_name + \"candlef1\", price_name + \"candlef2\",\n",
    "            price_name + \"candlef3\", price_name + \"candlef41\",\n",
    "            price_name + \"candlef42\"\n",
    "        ]\n",
    "\n",
    "        list_save = [i + \"_\" + flagname for i in list_save]\n",
    "        df_candle = cal_candlefactor(df_candle, price_name, vol_name, amt_name)\n",
    "\n",
    "        col_orig = list(df_candle.columns)\n",
    "        col_new = [i + \"_\" + flagname for i in col_orig]\n",
    "        df_candle.columns = col_new\n",
    "\n",
    "        data_merge_all = pd.merge(data_merge_all,\n",
    "                                  df_candle[list_save],\n",
    "                                  left_index=True,\n",
    "                                  right_index=True,\n",
    "                                  how=\"left\")\n",
    "\n",
    "    #加filter做candle\n",
    "    #    切割，打flag，给权重， 加filter切历史，波动率，分买入卖出，大单小单，上行下行，主买主卖\n",
    "    #    全天上行波动率/全天波动率\n",
    "    #index 数据   #复杂\n",
    "    #calculate max and min bid-ask spread\n",
    "    del data_merge_all[\"vol_orig\"]\n",
    "\n",
    "    return data_merge_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_lst = glob(os.path.join(input_dir,'book_test.parquet/*'))\n",
    "stock_lst = [os.path.basename(path).split('=')[-1] for path in path_lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name = pd.read_pickle(os.path.join(features_name_dir,'features_name_new.pkl')).values.reshape(-1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   time_id  stock_id  book_wap1<lambda>  book_wap2<lambda>  \\\n",
      "0        4         0           0.000294           0.000252   \n",
      "\n",
      "   book_wap_mean<lambda>  book_wap_diff<lambda>  book_price_spread<lambda>  \\\n",
      "0               0.000273                    0.0                   0.087097   \n",
      "\n",
      "   book_bid_spread<lambda>  book_ask_spread<lambda>  \\\n",
      "0                      0.0                      0.0   \n",
      "\n",
      "   book_total_volume<lambda>  ...  order_countsum  order_countmean  \\\n",
      "0                   0.314906  ...              11         3.666667   \n",
      "\n",
      "   order_countstd  order_countmax  order_countmin  seconds_in_bucketsum  \\\n",
      "0         3.05505               7               1                    58   \n",
      "\n",
      "   seconds_in_bucketmean  seconds_in_bucketstd  seconds_in_bucketmax  \\\n",
      "0              19.333333             10.785793                    27   \n",
      "\n",
      "   seconds_in_bucketmin  \n",
      "0                     7  \n",
      "\n",
      "[1 rows x 31 columns]\n",
      "isBS B\n",
      "isBS S\n",
      "isBS_big supB\n",
      "isBS_big supS\n",
      "isBS_big midBS\n",
      "isoversize50 up50\n",
      "isoversize50 down50\n",
      "isoversize25 up25\n",
      "isoversize25 down25\n",
      "isoversize75 up75\n",
      "isoversize75 down75\n",
      "         Bnum  waproll_std5_B_mean  waproll_std5_B_std  waproll_std5_B_skew  \\\n",
      "time_id                                                                       \n",
      "4           1                  NaN                 NaN                  NaN   \n",
      "\n",
      "         waproll_std5_B_autocorr  waproll_std10_B_mean  waproll_std10_B_std  \\\n",
      "time_id                                                                       \n",
      "4                            NaN                   NaN                  NaN   \n",
      "\n",
      "         waproll_std10_B_skew  waproll_std10_B_autocorr  wapcandlef1_B  ...  \\\n",
      "time_id                                                                 ...   \n",
      "4                         NaN                       NaN            0.0  ...   \n",
      "\n",
      "         waproll_std5_down75_autocorr  waproll_std10_down75_mean  \\\n",
      "time_id                                                            \n",
      "4                                 NaN                        NaN   \n",
      "\n",
      "         waproll_std10_down75_std  waproll_std10_down75_skew  \\\n",
      "time_id                                                        \n",
      "4                             NaN                        NaN   \n",
      "\n",
      "         waproll_std10_down75_autocorr  wapcandlef1_down75  \\\n",
      "time_id                                                      \n",
      "4                                  NaN                 0.0   \n",
      "\n",
      "         wapcandlef2_down75  wapcandlef3_down75  wapcandlef41_down75  \\\n",
      "time_id                                                                \n",
      "4                       NaN                 NaN                  0.0   \n",
      "\n",
      "         wapcandlef42_down75  \n",
      "time_id                       \n",
      "4                        0.0  \n",
      "\n",
      "[1 rows x 154 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   time_id  Bnum  waproll_std5_B_mean  waproll_std5_B_std  \\\n",
      "0        4     1                  NaN                 NaN   \n",
      "\n",
      "   waproll_std5_B_skew  waproll_std5_B_autocorr  waproll_std10_B_mean  \\\n",
      "0                  NaN                      NaN                   NaN   \n",
      "\n",
      "   waproll_std10_B_std  waproll_std10_B_skew  waproll_std10_B_autocorr  ...  \\\n",
      "0                  NaN                   NaN                       NaN  ...   \n",
      "\n",
      "   order_countsum  order_countmean  order_countstd  order_countmax  \\\n",
      "0              11         3.666667         3.05505               7   \n",
      "\n",
      "   order_countmin  seconds_in_bucketsum  seconds_in_bucketmean  \\\n",
      "0               1                    58              19.333333   \n",
      "\n",
      "   seconds_in_bucketstd  seconds_in_bucketmax  seconds_in_bucketmin  \n",
      "0             10.785793                    27                     7  \n",
      "\n",
      "[1 rows x 185 columns]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_ret_test = gen_data_multi(stock_lst, data_type='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_test = gen_data_encoding(df_ret_test, None, data_type = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:03,  2.95it/s]\n"
     ]
    }
   ],
   "source": [
    "y_preds = np.zeros(len(df_all_test))\n",
    "model_lst = glob(os.path.join(model_dir,'*model*.pkl'))\n",
    "for i, model_path in tqdm(enumerate(model_lst)):\n",
    "    model = joblib.load(model_path)\n",
    "    y_preds += model.predict(df_all_test[feature_name])\n",
    "y_preds /= (i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_test['row_id'] = df_all_test['stock_id'].astype(str)+'-'+df_all_test['time_id'].astype(str)\n",
    "df_all_test['target'] = y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = df_all_test[['row_id','target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('submission.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
