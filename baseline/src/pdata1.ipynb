{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils.util' from '/Users/wanjun/Desktop/比赛训练营/Kaggle-量化比赛/code/src/utils/util.py'>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "from glob import glob\n",
    "from imp import reload\n",
    "import copy as cp\n",
    "from utils import util\n",
    "reload(util)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path_lst = glob('../data/book_train.parquet/*')\n",
    "stock_lst = [os.path.basename(path).split('=')[-1] for path in path_lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wanjun/anaconda/envs/python36/lib/python3.6/site-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/wanjun/anaconda/envs/python36/lib/python3.6/site-packages/pandas/core/series.py:726: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/wanjun/anaconda/envs/python36/lib/python3.6/site-packages/pandas/core/algorithms.py:2001: RuntimeWarning: invalid value encountered in subtract\n",
      "  out_arr[res_indexer] = arr[res_indexer] - arr[lag_indexer]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isBS B\n",
      "isBS S\n",
      "isBS_big supB\n",
      "isBS_big supS\n",
      "isBS_big midBS\n",
      "isoversize50 up50\n",
      "isoversize50 down50\n",
      "isoversize25 up25\n",
      "isoversize25 down25\n",
      "isoversize75 up75\n",
      "isoversize75 down75\n"
     ]
    }
   ],
   "source": [
    "temp = util.gen_data_train(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/112 [00:00<?, ?it/s]/Users/wanjun/anaconda/envs/python36/lib/python3.6/site-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/wanjun/anaconda/envs/python36/lib/python3.6/site-packages/pandas/core/series.py:726: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/wanjun/anaconda/envs/python36/lib/python3.6/site-packages/pandas/core/algorithms.py:2001: RuntimeWarning: invalid value encountered in subtract\n",
      "  out_arr[res_indexer] = arr[res_indexer] - arr[lag_indexer]\n",
      "/Users/wanjun/anaconda/envs/python36/lib/python3.6/site-packages/pandas/core/series.py:726: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/wanjun/anaconda/envs/python36/lib/python3.6/site-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/wanjun/anaconda/envs/python36/lib/python3.6/site-packages/pandas/core/algorithms.py:2001: RuntimeWarning: invalid value encountered in subtract\n",
      "  out_arr[res_indexer] = arr[res_indexer] - arr[lag_indexer]\n",
      "/Users/wanjun/anaconda/envs/python36/lib/python3.6/site-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/wanjun/anaconda/envs/python36/lib/python3.6/site-packages/pandas/core/series.py:726: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/wanjun/anaconda/envs/python36/lib/python3.6/site-packages/pandas/core/algorithms.py:2001: RuntimeWarning: invalid value encountered in subtract\n",
      "  out_arr[res_indexer] = arr[res_indexer] - arr[lag_indexer]\n",
      "/Users/wanjun/anaconda/envs/python36/lib/python3.6/site-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/wanjun/anaconda/envs/python36/lib/python3.6/site-packages/pandas/core/series.py:726: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/wanjun/anaconda/envs/python36/lib/python3.6/site-packages/pandas/core/algorithms.py:2001: RuntimeWarning: invalid value encountered in subtract\n",
      "  out_arr[res_indexer] = arr[res_indexer] - arr[lag_indexer]\n",
      "/Users/wanjun/anaconda/envs/python36/lib/python3.6/site-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/wanjun/anaconda/envs/python36/lib/python3.6/site-packages/pandas/core/series.py:726: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/wanjun/anaconda/envs/python36/lib/python3.6/site-packages/pandas/core/algorithms.py:2001: RuntimeWarning: invalid value encountered in subtract\n",
      "  out_arr[res_indexer] = arr[res_indexer] - arr[lag_indexer]\n",
      "/Users/wanjun/anaconda/envs/python36/lib/python3.6/site-packages/pandas/core/series.py:726: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/wanjun/anaconda/envs/python36/lib/python3.6/site-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/wanjun/anaconda/envs/python36/lib/python3.6/site-packages/pandas/core/algorithms.py:2001: RuntimeWarning: invalid value encountered in subtract\n",
      "  out_arr[res_indexer] = arr[res_indexer] - arr[lag_indexer]\n",
      "/Users/wanjun/anaconda/envs/python36/lib/python3.6/site-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/wanjun/anaconda/envs/python36/lib/python3.6/site-packages/pandas/core/series.py:726: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/wanjun/anaconda/envs/python36/lib/python3.6/site-packages/pandas/core/algorithms.py:2001: RuntimeWarning: invalid value encountered in subtract\n",
      "  out_arr[res_indexer] = arr[res_indexer] - arr[lag_indexer]\n",
      "/Users/wanjun/anaconda/envs/python36/lib/python3.6/site-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/wanjun/anaconda/envs/python36/lib/python3.6/site-packages/pandas/core/series.py:726: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/wanjun/anaconda/envs/python36/lib/python3.6/site-packages/pandas/core/algorithms.py:2001: RuntimeWarning: invalid value encountered in subtract\n",
      "  out_arr[res_indexer] = arr[res_indexer] - arr[lag_indexer]\n",
      "/Users/wanjun/anaconda/envs/python36/lib/python3.6/site-packages/pandas/core/reshape/merge.py:643: UserWarning: merging between different levels can give an unintended result (1 levels on the left,2 on the right)\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/Users/wanjun/anaconda/envs/python36/lib/python3.6/site-packages/pandas/core/generic.py:3889: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n",
      "/Users/wanjun/anaconda/envs/python36/lib/python3.6/site-packages/pandas/core/reshape/merge.py:643: UserWarning: merging between different levels can give an unintended result (1 levels on the left,2 on the right)\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/Users/wanjun/anaconda/envs/python36/lib/python3.6/site-packages/pandas/core/generic.py:3889: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n",
      "/Users/wanjun/anaconda/envs/python36/lib/python3.6/site-packages/pandas/core/reshape/merge.py:643: UserWarning: merging between different levels can give an unintended result (1 levels on the left,2 on the right)\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/Users/wanjun/anaconda/envs/python36/lib/python3.6/site-packages/pandas/core/generic.py:3889: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n",
      "/Users/wanjun/anaconda/envs/python36/lib/python3.6/site-packages/pandas/core/reshape/merge.py:643: UserWarning: merging between different levels can give an unintended result (1 levels on the left,2 on the right)\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/Users/wanjun/anaconda/envs/python36/lib/python3.6/site-packages/pandas/core/generic.py:3889: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n",
      "/Users/wanjun/anaconda/envs/python36/lib/python3.6/site-packages/pandas/core/reshape/merge.py:643: UserWarning: merging between different levels can give an unintended result (1 levels on the left,2 on the right)\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/Users/wanjun/anaconda/envs/python36/lib/python3.6/site-packages/pandas/core/generic.py:3889: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n",
      "/Users/wanjun/anaconda/envs/python36/lib/python3.6/site-packages/pandas/core/reshape/merge.py:643: UserWarning: merging between different levels can give an unintended result (1 levels on the left,2 on the right)\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/Users/wanjun/anaconda/envs/python36/lib/python3.6/site-packages/pandas/core/generic.py:3889: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n",
      "  1%|          | 1/112 [00:42<1:18:56, 42.67s/it]/Users/wanjun/anaconda/envs/python36/lib/python3.6/site-packages/pandas/core/reshape/merge.py:643: UserWarning: merging between different levels can give an unintended result (1 levels on the left,2 on the right)\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/Users/wanjun/anaconda/envs/python36/lib/python3.6/site-packages/pandas/core/generic.py:3889: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n",
      "/Users/wanjun/anaconda/envs/python36/lib/python3.6/site-packages/pandas/core/reshape/merge.py:643: UserWarning: merging between different levels can give an unintended result (1 levels on the left,2 on the right)\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/Users/wanjun/anaconda/envs/python36/lib/python3.6/site-packages/pandas/core/generic.py:3889: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n",
      "100%|██████████| 112/112 [09:52<00:00,  5.29s/it]\n"
     ]
    }
   ],
   "source": [
    "data_type = 'train'\n",
    "fe_df, stock_df = util.gen_data_all(stock_lst, data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df.to_pickle('../data/train_stock_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.001445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.002168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0.002195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.001747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stock_id  time_id    target\n",
       "0         0        5  0.004136\n",
       "1         0       11  0.001445\n",
       "2         0       16  0.002168\n",
       "3         0       31  0.002195\n",
       "4         0       62  0.001747"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fe_df = fe_df.merge(\n",
    "        stock_df\n",
    "        , how='left'\n",
    "        , on='stock_id'\n",
    "    ).merge(\n",
    "        train\n",
    "        , how='left'\n",
    "        , on=['stock_id', 'time_id']\n",
    "    ).replace([np.inf, -np.inf], np.nan).fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name in tqdm(name_lst):\n",
    "#     ret = util.gen_data(name)\n",
    "#     ret.to_csv('../data/20210731/{}.csv'.format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  lightgbm  as lgb\n",
    "\n",
    "# LightGBM parameters\n",
    "params = {\n",
    "    'n_estimators': 10000,\n",
    "    'objective': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'max_depth': -1,\n",
    "    'learning_rate': 0.01,\n",
    "    'subsample': 0.72,\n",
    "    'subsample_freq': 4,\n",
    "    'feature_fraction': 0.8,\n",
    "    'lambda_l1': 1,\n",
    "    'lambda_l2': 1,\n",
    "    'seed': 46,\n",
    "    'early_stopping_rounds': 300,\n",
    "    'verbose': -1\n",
    "} \n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSPEMetric(XGBoost=False):\n",
    "    def RMSPE(yhat, dtrain, XGBoost=XGBoost):\n",
    "\n",
    "        y = dtrain.get_label()\n",
    "        elements = ((y - yhat) / y)**2\n",
    "        if XGBoost:\n",
    "            return 'RMSPE', float(np.sqrt(np.sum(elements) / len(y)))\n",
    "        else:\n",
    "            return 'RMSPE', float(np.sqrt(np.sum(elements) / len(y))), False\n",
    "\n",
    "    return RMSPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fe_df\n",
    "label = fe_df['target']\n",
    "features = fe_df.columns.difference(['time_id','target']).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/Users/wanjun/anaconda/envs/python36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/Users/wanjun/anaconda/envs/python36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/Users/wanjun/anaconda/envs/python36/lib/python3.6/site-packages/lightgbm/basic.py:1702: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "/Users/wanjun/anaconda/envs/python36/lib/python3.6/site-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/Users/wanjun/anaconda/envs/python36/lib/python3.6/site-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 300 rounds\n",
      "[250]\ttraining's rmse: 0.000507154\ttraining's RMSPE: 0.234868\tvalid_1's rmse: 0.00051562\tvalid_1's RMSPE: 0.237253\n",
      "[500]\ttraining's rmse: 0.000478376\ttraining's RMSPE: 0.22154\tvalid_1's rmse: 0.000491263\tvalid_1's RMSPE: 0.226046\n",
      "[750]\ttraining's rmse: 0.000468124\ttraining's RMSPE: 0.216793\tvalid_1's rmse: 0.000483037\tvalid_1's RMSPE: 0.222261\n",
      "[1000]\ttraining's rmse: 0.000460173\ttraining's RMSPE: 0.21311\tvalid_1's rmse: 0.000476741\tvalid_1's RMSPE: 0.219364\n",
      "[1250]\ttraining's rmse: 0.000454155\ttraining's RMSPE: 0.210324\tvalid_1's rmse: 0.000472235\tvalid_1's RMSPE: 0.217291\n",
      "[1500]\ttraining's rmse: 0.000448798\ttraining's RMSPE: 0.207843\tvalid_1's rmse: 0.000468512\tvalid_1's RMSPE: 0.215577\n",
      "[1750]\ttraining's rmse: 0.000443858\ttraining's RMSPE: 0.205555\tvalid_1's rmse: 0.000464965\tvalid_1's RMSPE: 0.213945\n",
      "[2000]\ttraining's rmse: 0.000439789\ttraining's RMSPE: 0.203671\tvalid_1's rmse: 0.000462325\tvalid_1's RMSPE: 0.212731\n",
      "[2250]\ttraining's rmse: 0.000436359\ttraining's RMSPE: 0.202082\tvalid_1's rmse: 0.000460354\tvalid_1's RMSPE: 0.211824\n",
      "[2500]\ttraining's rmse: 0.000432978\ttraining's RMSPE: 0.200516\tvalid_1's rmse: 0.000458234\tvalid_1's RMSPE: 0.210848\n",
      "[2750]\ttraining's rmse: 0.000429949\ttraining's RMSPE: 0.199113\tvalid_1's rmse: 0.000456421\tvalid_1's RMSPE: 0.210014\n",
      "[3000]\ttraining's rmse: 0.000427138\ttraining's RMSPE: 0.197812\tvalid_1's rmse: 0.000454752\tvalid_1's RMSPE: 0.209246\n",
      "[3250]\ttraining's rmse: 0.0004245\ttraining's RMSPE: 0.19659\tvalid_1's rmse: 0.000453336\tvalid_1's RMSPE: 0.208595\n",
      "[3500]\ttraining's rmse: 0.000421924\ttraining's RMSPE: 0.195397\tvalid_1's rmse: 0.000451949\tvalid_1's RMSPE: 0.207956\n",
      "[3750]\ttraining's rmse: 0.000419591\ttraining's RMSPE: 0.194316\tvalid_1's rmse: 0.000450733\tvalid_1's RMSPE: 0.207397\n",
      "[4000]\ttraining's rmse: 0.000417249\ttraining's RMSPE: 0.193232\tvalid_1's rmse: 0.000449497\tvalid_1's RMSPE: 0.206828\n",
      "[4250]\ttraining's rmse: 0.000415082\ttraining's RMSPE: 0.192228\tvalid_1's rmse: 0.000448428\tvalid_1's RMSPE: 0.206336\n",
      "[4500]\ttraining's rmse: 0.000413008\ttraining's RMSPE: 0.191268\tvalid_1's rmse: 0.000447415\tvalid_1's RMSPE: 0.20587\n",
      "[4750]\ttraining's rmse: 0.000410982\ttraining's RMSPE: 0.19033\tvalid_1's rmse: 0.000446363\tvalid_1's RMSPE: 0.205386\n",
      "[5000]\ttraining's rmse: 0.000408974\ttraining's RMSPE: 0.1894\tvalid_1's rmse: 0.00044542\tvalid_1's RMSPE: 0.204952\n",
      "[5250]\ttraining's rmse: 0.000407032\ttraining's RMSPE: 0.1885\tvalid_1's rmse: 0.000444476\tvalid_1's RMSPE: 0.204518\n",
      "[5500]\ttraining's rmse: 0.00040522\ttraining's RMSPE: 0.187661\tvalid_1's rmse: 0.000443693\tvalid_1's RMSPE: 0.204157\n",
      "[5750]\ttraining's rmse: 0.000403407\ttraining's RMSPE: 0.186822\tvalid_1's rmse: 0.000442869\tvalid_1's RMSPE: 0.203778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [05:30, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-2dad2336b60a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m                       \u001b[0mvalid_sets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                       \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRMSPEMetric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                       verbose_eval=250)\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# feature importance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/python36/lib/python3.6/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    247\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/python36/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   2643\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   2644\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2645\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   2646\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2647\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cats = ['stock_id', ]\n",
    "X_train = data_.reset_index(drop=True)\n",
    "y_train = label\n",
    "# y_train = pd.DataFrame(label_)\n",
    "models = []\n",
    "oof_df = X_train[['time_id', 'stock_id']].copy()\n",
    "oof_df['target'] = y_train\n",
    "oof_df['pred'] = np.nan\n",
    "\n",
    "cv = model_selection.KFold(n_splits=10,\n",
    "                            shuffle=True,\n",
    "                            random_state=666)\n",
    "\n",
    "kf = cv.split(X_train, y_train)\n",
    "\n",
    "fi_df = pd.DataFrame()\n",
    "fi_df['features'] = features\n",
    "fi_df['importance'] = 0\n",
    "\n",
    "for fold_id, (train_index, valid_index) in tqdm(enumerate(kf)):\n",
    "    # split\n",
    "    X_tr = X_train.loc[train_index, features]\n",
    "    X_val = X_train.loc[valid_index, features]\n",
    "    y_tr = y_train.loc[train_index].values.reshape(-1)\n",
    "    y_val = y_train.loc[valid_index].values.reshape(-1)\n",
    "\n",
    "    # model (note inverse weighting)\n",
    "    train_set = lgb.Dataset(X_tr,\n",
    "                            y_tr,\n",
    "                            categorical_feature=cats,\n",
    "                            weight=1 / np.power(y_tr, 2))\n",
    "    val_set = lgb.Dataset(X_val,\n",
    "                          y_val,\n",
    "                          categorical_feature=cats,\n",
    "                          weight=1 / np.power(y_val, 2))\n",
    "    model = lgb.train(params,\n",
    "                      train_set,\n",
    "                      valid_sets=[train_set, val_set],\n",
    "                      feval=RMSPEMetric(),\n",
    "                      verbose_eval=250)\n",
    "\n",
    "    # feature importance\n",
    "    fi_df[f'importance_fold{fold_id}'] = model.feature_importance(\n",
    "        importance_type=\"gain\")\n",
    "    fi_df['importance'] += fi_df[f'importance_fold{fold_id}'].values\n",
    "\n",
    "    # save model\n",
    "    #joblib.dump(model, f'model_fold{fold_id}.pkl')\n",
    "    #logger.debug('model saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rollingstats(rolling_x, roll_name):\n",
    "    #统计量\n",
    "    roll_autocorr =  rolling_x.groupby(\"time_id\")[[roll_name,\"xpre\"]].corr()\n",
    "    roll_autocorr .reset_index(inplace=True)\n",
    "    roll_autocorr = roll_autocorr.groupby(\"time_id\").head(1)\n",
    "    roll_autocorr.index = roll_autocorr[\"time_id\"]\n",
    "    del roll_autocorr[\"time_id\"]\n",
    "    roll_autocorr = pd.DataFrame({roll_name+\"_autocorr\": roll_autocorr[\"xpre\"]})\n",
    "    \n",
    "    roll_mean = pd.DataFrame({roll_name+\"_mean\": rolling_x.groupby(\"time_id\")[roll_name].mean()})\n",
    "    roll_std = pd.DataFrame({roll_name+\"_std\": rolling_x.groupby(\"time_id\")[roll_name].std()})\n",
    "    roll_skew = pd.DataFrame({roll_name+\"_skew\": rolling_x.groupby(\"time_id\")[roll_name].skew()})\n",
    "    \n",
    "    data_merge = pd.merge(roll_mean, roll_std, left_index=True, right_index=True,how=\"inner\")\n",
    "    data_merge = pd.merge(data_merge , roll_skew, left_index=True, right_index=True,how=\"inner\")\n",
    "    data_merge = pd.merge(data_merge , roll_autocorr, left_index=True, right_index=True,how=\"inner\")\n",
    "    return data_merge\n",
    "\n",
    "def make_candle(df_data, price_name, vol_name, amt_name):\n",
    "    \n",
    "    df_data[\"pre\"] = df_data.groupby(\"time_id\")[price_name].shift(1)\n",
    "    df_data[\"ret\"] = df_data[price_name] / df_data[\"pre\"] - 1\n",
    "    df_data[\"absret\"] = abs(df_data[\"ret\"] )\n",
    "    df_retsum = pd.DataFrame({\"retsum\":df_data.groupby(\"time_id\")[\"ret\"].sum()})\n",
    "    df_absretsum = pd.DataFrame({\"absretsum\":df_data.groupby(\"time_id\")[\"absret\"].sum()})\n",
    "    \n",
    "    df_data[\"absobv\"] = df_data[\"absret\"] * df_data[vol_name]\n",
    "    df_obvabs = pd.DataFrame({\"xf4_abs\":df_data.groupby(\"time_id\")[\"absobv\"].sum()})\n",
    "    \n",
    "    df_data[\"obv\"] = df_data[\"ret\"] * df_data[vol_name]\n",
    "    df_obv = pd.DataFrame({\"xf4\":df_data.groupby(\"time_id\")[\"obv\"].sum()})\n",
    "    \n",
    "    df_amt =  pd.DataFrame({amt_name + \"sum\": df_data.groupby(\"time_id\")[amt_name].sum()})\n",
    "    df_vol =  pd.DataFrame({vol_name + \"sum\": df_data.groupby(\"time_id\")[vol_name].sum()})\n",
    "    \n",
    "    df_mean = pd.DataFrame({price_name + \"mean\": df_data.groupby(\"time_id\")[price_name].mean()})\n",
    "    df_high = pd.DataFrame({price_name + \"high\": df_data.groupby(\"time_id\")[price_name].max()})\n",
    "    df_low = pd.DataFrame({price_name + \"low\": df_data.groupby(\"time_id\")[price_name].min()})\n",
    "    \n",
    "    df_open = df_data.groupby(\"time_id\").head(1)\n",
    "    df_open.index = df_open [\"time_id\"]\n",
    "    df_open = pd.DataFrame({price_name + \"open\": df_open[price_name]})\n",
    "    \n",
    "    df_close = df_data.groupby(\"time_id\").tail(1)\n",
    "    df_close.index = df_close[\"time_id\"]\n",
    "    df_close = pd.DataFrame({price_name + \"close\": df_close[price_name]})\n",
    "    \n",
    "    df_candle = pd.merge(df_high, df_low, left_index=True, right_index=True,how=\"inner\")\n",
    "    df_candle = pd.merge(df_candle, df_mean, left_index=True, right_index=True,how=\"inner\")\n",
    "    df_candle = pd.merge(df_candle, df_open, left_index=True, right_index=True,how=\"inner\")\n",
    "    df_candle = pd.merge(df_candle, df_close, left_index=True, right_index=True,how=\"inner\")\n",
    "    df_candle = pd.merge(df_candle, df_vol, left_index=True, right_index=True,how=\"inner\")\n",
    "    df_candle = pd.merge(df_candle, df_amt, left_index=True, right_index=True,how=\"inner\")    \n",
    "    df_candle = pd.merge(df_candle, df_retsum, left_index=True, right_index=True,how=\"inner\")    \n",
    "    df_candle = pd.merge(df_candle, df_absretsum, left_index=True, right_index=True,how=\"inner\")    \n",
    "    df_candle = pd.merge(df_candle, df_obvabs, left_index=True, right_index=True,how=\"inner\")    \n",
    "    df_candle = pd.merge(df_candle, df_obv, left_index=True, right_index=True,how=\"inner\")    \n",
    "    \n",
    "    return df_candle \n",
    "\n",
    "def cal_candlefactor(df_candle, price_name, vol_name, amt_name):\n",
    "    f_name = price_name + \"candle\"\n",
    "    #f1:illiq\n",
    "    df_candle[f_name + \"f1\"] = (2 * (df_candle[price_name + \"high\"] - df_candle[price_name + \"low\"]) \n",
    "                    - abs(df_candle[price_name + \"open\"] - df_candle[price_name + \"close\"]))/df_candle[amt_name + \"sum\"]\n",
    "    #f2 strength\n",
    "    df_candle[f_name + \"f2\"] = df_candle[\"retsum\"]/df_candle[\"absretsum\"]\n",
    "    #f3:ad\n",
    "    df_candle[f_name + \"f3\"] =  (2 *df_candle[price_name + \"close\"] - df_candle[price_name + \"low\"]\\\n",
    "                    - df_candle[price_name + \"high\"] )/(df_candle[price_name + \"high\"] - df_candle[price_name + \"low\"]) \\\n",
    "                    * df_candle[vol_name + \"sum\"]\n",
    "    #f3: obv\n",
    "    df_candle[f_name + \"f41\"] =  df_candle[\"xf4\"]/df_candle[vol_name + \"sum\"]\n",
    "    df_candle[f_name + \"f42\"] =  df_candle[\"xf4_abs\"]/df_candle[vol_name + \"sum\"]\n",
    "    return df_candle\n",
    "\n",
    "# def calc_rollingstats_realvol(rolling_x, roll_name):\n",
    "#     #统计量\n",
    "#     vol_new =  pd.DataFrame({roll_name+\"_autocorr\": rolling_x.groupby('time_id')[roll_name].apply(\n",
    "#                                     lambda x: np.sqrt(np.sum(np.log(x).diff()**2)))})\n",
    "#     data_merge = pd.merge(data_merge , vol_new, left_index=True, right_index=True,how=\"inner\")\n",
    "#     return data_merge\n",
    "\n",
    "def calculate_features2(book_df, trade_df):\n",
    "    \"\"\"\n",
    "    df: book_train data for each stock_id\n",
    "    \"\"\"\n",
    "    #calculate price for features\n",
    "    \n",
    "    book_df['wap'] = (book_df['bid_price1'] * book_df['ask_size1'] + book_df['ask_price1'] * \n",
    "                       book_df['bid_size1']) / (book_df['bid_size1']+ book_df['ask_size1'])\n",
    "    \n",
    "    book_df[\"vol_ab\"] = book_df['bid_size1']+ book_df['ask_size1']\n",
    "    book_df[\"amt_ab\"] = book_df['bid_price1'] * book_df['ask_size1'] + book_df['ask_price1'] * book_df['bid_size1']\n",
    "    \n",
    "    book_df[\"amt_a\"] = book_df['ask_price1'] * book_df['ask_size1'] \n",
    "    book_df[\"amt_b\"] = book_df['bid_price1'] * book_df['bid_size1'] \n",
    "    \n",
    "    trade_df[\"amt\"] = trade_df[\"price\"] * trade_df[\"size\"]\n",
    "    \n",
    "    #flag filter\n",
    "    book_df[\"wap_pre\"] = book_df.groupby(\"time_id\")['wap'].shift(1)\n",
    "    book_df[\"bid_ppre\"] = book_df.groupby(\"time_id\")['bid_price1'].shift(1)\n",
    "    book_df[\"ask_ppre\"] = book_df.groupby(\"time_id\")['ask_price1'].shift(1)\n",
    "    \n",
    "    \n",
    "    book_df[\"isBS\"] = np.where(book_df[\"wap\"]>book_df[\"wap_pre\"], \"B\",\n",
    "                       np.where(book_df[\"wap\"]<book_df[\"wap_pre\"], \"S\",np.nan))\n",
    "    book_df[\"isBS_big\"] = np.where(book_df[\"wap\"]>book_df[\"ask_ppre\"], \"supB\",\n",
    "                       np.where(book_df[\"wap\"]<book_df[\"bid_ppre\"], \"supS\",\n",
    "                        np.where(pd.notnull(book_df[\"wap\"]),\"midBS\",np.nan)))\n",
    "    \n",
    "    ordersize50 = pd.DataFrame({\"ordersize50\":book_df.groupby(\"time_id\")[\"amt_ab\"].apply(lambda x :np.nanmedian(x))})\n",
    "    ordersize50.reset_index(inplace=True)\n",
    "    \n",
    "    ordersize25 = pd.DataFrame({\"ordersize25\": book_df.groupby(\"time_id\")[\"amt_ab\"].apply(lambda x :np.nanpercentile(x,75))})\n",
    "    ordersize25.reset_index(inplace=True)\n",
    "    \n",
    "    ordersize75 = pd.DataFrame({\"ordersize75\": book_df.groupby(\"time_id\")[\"amt_ab\"].apply(lambda x :np.nanpercentile(x, 25))}) \n",
    "    ordersize75.reset_index(inplace=True)\n",
    "    book_df1 = pd.merge(book_df, ordersize50, on=\"time_id\", how=\"left\")\n",
    "    book_df1 = pd.merge(book_df1, ordersize25, on=\"time_id\", how=\"left\")\n",
    "    book_df1 = pd.merge(book_df1, ordersize75, on=\"time_id\", how=\"left\")\n",
    "    book_df1.loc[:,\"isoversize50\"] = np.where(book_df1[\"amt_ab\"]>book_df1[\"ordersize50\"],\"up50\",\n",
    "                                     np.where(book_df1[\"amt_ab\"]<=book_df1[\"ordersize50\"],\"down50\",np.nan))\n",
    "    \n",
    "    \n",
    "    book_df1.loc[:,\"isoversize75\"] = np.where(book_df1[\"amt_ab\"]>book_df1[\"ordersize75\"],\"up75\",\n",
    "                                     np.where(book_df1[\"amt_ab\"]<=book_df1[\"ordersize75\"],\"down75\",np.nan))\n",
    "    book_df1.loc[:,\"isoversize25\"] = np.where(book_df1[\"amt_ab\"]>book_df1[\"ordersize25\"],\"up25\",\n",
    "                                     np.where(book_df1[\"amt_ab\"]<=book_df1[\"ordersize25\"],\"down25\",np.nan))\n",
    "    \n",
    "     #不同波动率\n",
    "    #calculate historical volatility\n",
    "    vol = book_df1.groupby('time_id')['wap'].apply(lambda x: np.sqrt(np.sum(np.log(x).diff()**2)))\n",
    "    vol_df = pd.DataFrame(vol)\n",
    "    vol_df.rename(columns={'wap': 'vol_orig'}, inplace=True)\n",
    "    data_merge_all = vol_df\n",
    "        \n",
    "    #修改波动率：\n",
    "#    rolling波动率均值，标准差，偏度，自相关\n",
    "    #新指标,新指标均值，标准差，偏度，自相关\n",
    "\n",
    "#    roll_name0 = \"roll_std\" \n",
    "#    roll_window = 10\n",
    "    #BS FLAG\n",
    "    flagname = \"B\"\n",
    "    filtername = \"isBS\"\n",
    "    for  filtername, flagname in [[\"isBS\",\"B\"],[\"isBS\",\"S\"],\n",
    "                                  [\"isBS_big\",\"supB\"],[\"isBS_big\",\"supS\"],[\"isBS_big\",\"midBS\"],\n",
    "                                  [\"isoversize50\",\"up50\"],[\"isoversize50\",\"down50\"],\n",
    "                                  [\"isoversize25\",\"up25\"],[\"isoversize25\",\"down25\"],\n",
    "                                  [\"isoversize75\",\"up75\"],[\"isoversize75\",\"down75\"]]:\n",
    "        print(filtername, flagname)\n",
    "        book_df_new = book_df1[book_df1[filtername] == flagname]\n",
    "         #个数\n",
    "        df_fnum = pd.DataFrame({flagname + \"num\":book_df_new .groupby(\"time_id\")[\"seconds_in_bucket\"].count()})\n",
    "        data_merge_all =  pd.merge(data_merge_all, df_fnum, left_index=True, right_index=True,how=\"left\")\n",
    "    \n",
    "    \n",
    "        for roll_window in [5, 10]:\n",
    "            #rolling指标\n",
    "            price_name = \"wap\"\n",
    "            roll_name0 = price_name + \"roll_std\" \n",
    "            roll_name = roll_name0 + str(roll_window)+\"_\"+flagname\n",
    "            \n",
    "            rolling_x = pd.DataFrame({roll_name:book_df_new.groupby(\"time_id\")[price_name].rolling(roll_window).std()})\n",
    "            rolling_x.reset_index(inplace=True)\n",
    "            rolling_x.loc[:,\"xpre\"] = rolling_x.groupby(\"time_id\")[roll_name].shift(1)\n",
    "            #计算统计量因子\n",
    "            data_merge = calc_rollingstats(rolling_x, roll_name)\n",
    "            data_merge_all =  pd.merge(data_merge_all, data_merge, left_index=True, right_index=True,how=\"left\")\n",
    "        \n",
    "        \n",
    "        #全局做candle：wap，买盘，卖盘\n",
    "        #candle因子\n",
    "        price_name = \"wap\"\n",
    "        vol_name = \"vol_ab\"\n",
    "        amt_name = \"amt_ab\"\n",
    "        df_data = cp.deepcopy(book_df_new)\n",
    "        df_candle  = make_candle(df_data, price_name, vol_name, amt_name)\n",
    "        \n",
    "        list_save = [price_name + \"candlef1\", price_name + \"candlef2\", price_name + \"candlef3\",\n",
    "                     price_name + \"candlef41\", price_name + \"candlef42\"]\n",
    "       \n",
    "        list_save = [i +\"_\"+flagname for i in list_save ]\n",
    "        df_candle = cal_candlefactor(df_candle, price_name, vol_name, amt_name)\n",
    "        \n",
    "        col_orig = list(df_candle.columns)\n",
    "        col_new = [i +\"_\"+flagname for i in col_orig ]\n",
    "        df_candle.columns = col_new\n",
    "        \n",
    "        data_merge_all = pd.merge(data_merge_all, df_candle[list_save],left_index=True, right_index=True,how=\"left\")\n",
    "\n",
    "    #加filter做candle\n",
    "    \n",
    "#    切割，打flag，给权重， 加filter切历史，波动率，分买入卖出，大单小单，上行下行，主买主卖\n",
    "#    全天上行波动率/全天波动率    \n",
    "    #index 数据   #复杂    \n",
    "    #calculate max and min bid-ask spread\n",
    "    del data_merge_all[\"vol_orig\"]\n",
    "    \n",
    "    return data_merge_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "book0 = util.load_book(0)\n",
    "trade0 = util.load_trade(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isBS B\n",
      "isBS S\n",
      "isBS_big supB\n",
      "isBS_big supS\n",
      "isBS_big midBS\n",
      "isoversize50 up50\n",
      "isoversize50 down50\n",
      "isoversize25 up25\n",
      "isoversize25 down25\n",
      "isoversize75 up75\n",
      "isoversize75 down75\n"
     ]
    }
   ],
   "source": [
    "temp = calculate_features2(book0,trade0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_id\n",
      "Bnum\n",
      "waproll_std5_B_mean\n",
      "waproll_std5_B_std\n",
      "waproll_std5_B_skew\n",
      "waproll_std5_B_autocorr\n",
      "waproll_std10_B_mean\n",
      "waproll_std10_B_std\n",
      "waproll_std10_B_skew\n",
      "waproll_std10_B_autocorr\n",
      "wapcandlef1_B\n",
      "wapcandlef2_B\n",
      "wapcandlef3_B\n",
      "wapcandlef41_B\n",
      "wapcandlef42_B\n",
      "Snum\n",
      "waproll_std5_S_mean\n",
      "waproll_std5_S_std\n",
      "waproll_std5_S_skew\n",
      "waproll_std5_S_autocorr\n",
      "waproll_std10_S_mean\n",
      "waproll_std10_S_std\n",
      "waproll_std10_S_skew\n",
      "waproll_std10_S_autocorr\n",
      "wapcandlef1_S\n",
      "wapcandlef2_S\n",
      "wapcandlef3_S\n",
      "wapcandlef41_S\n",
      "wapcandlef42_S\n",
      "supBnum\n",
      "waproll_std5_supB_mean\n",
      "waproll_std5_supB_std\n",
      "waproll_std5_supB_skew\n",
      "waproll_std5_supB_autocorr\n",
      "waproll_std10_supB_mean\n",
      "waproll_std10_supB_std\n",
      "waproll_std10_supB_skew\n",
      "waproll_std10_supB_autocorr\n",
      "wapcandlef1_supB\n",
      "wapcandlef2_supB\n",
      "wapcandlef3_supB\n",
      "wapcandlef41_supB\n",
      "wapcandlef42_supB\n",
      "supSnum\n",
      "waproll_std5_supS_mean\n",
      "waproll_std5_supS_std\n",
      "waproll_std5_supS_skew\n",
      "waproll_std5_supS_autocorr\n",
      "waproll_std10_supS_mean\n",
      "waproll_std10_supS_std\n",
      "waproll_std10_supS_skew\n",
      "waproll_std10_supS_autocorr\n",
      "wapcandlef1_supS\n",
      "wapcandlef2_supS\n",
      "wapcandlef3_supS\n",
      "wapcandlef41_supS\n",
      "wapcandlef42_supS\n",
      "midBSnum\n",
      "waproll_std5_midBS_mean\n",
      "waproll_std5_midBS_std\n",
      "waproll_std5_midBS_skew\n",
      "waproll_std5_midBS_autocorr\n",
      "waproll_std10_midBS_mean\n",
      "waproll_std10_midBS_std\n",
      "waproll_std10_midBS_skew\n",
      "waproll_std10_midBS_autocorr\n",
      "wapcandlef1_midBS\n",
      "wapcandlef2_midBS\n",
      "wapcandlef3_midBS\n",
      "wapcandlef41_midBS\n",
      "wapcandlef42_midBS\n",
      "up50num\n",
      "waproll_std5_up50_mean\n",
      "waproll_std5_up50_std\n",
      "waproll_std5_up50_skew\n",
      "waproll_std5_up50_autocorr\n",
      "waproll_std10_up50_mean\n",
      "waproll_std10_up50_std\n",
      "waproll_std10_up50_skew\n",
      "waproll_std10_up50_autocorr\n",
      "wapcandlef1_up50\n",
      "wapcandlef2_up50\n",
      "wapcandlef3_up50\n",
      "wapcandlef41_up50\n",
      "wapcandlef42_up50\n",
      "down50num\n",
      "waproll_std5_down50_mean\n",
      "waproll_std5_down50_std\n",
      "waproll_std5_down50_skew\n",
      "waproll_std5_down50_autocorr\n",
      "waproll_std10_down50_mean\n",
      "waproll_std10_down50_std\n",
      "waproll_std10_down50_skew\n",
      "waproll_std10_down50_autocorr\n",
      "wapcandlef1_down50\n",
      "wapcandlef2_down50\n",
      "wapcandlef3_down50\n",
      "wapcandlef41_down50\n",
      "wapcandlef42_down50\n",
      "up25num\n",
      "waproll_std5_up25_mean\n",
      "waproll_std5_up25_std\n",
      "waproll_std5_up25_skew\n",
      "waproll_std5_up25_autocorr\n",
      "waproll_std10_up25_mean\n",
      "waproll_std10_up25_std\n",
      "waproll_std10_up25_skew\n",
      "waproll_std10_up25_autocorr\n",
      "wapcandlef1_up25\n",
      "wapcandlef2_up25\n",
      "wapcandlef3_up25\n",
      "wapcandlef41_up25\n",
      "wapcandlef42_up25\n",
      "down25num\n",
      "waproll_std5_down25_mean\n",
      "waproll_std5_down25_std\n",
      "waproll_std5_down25_skew\n",
      "waproll_std5_down25_autocorr\n",
      "waproll_std10_down25_mean\n",
      "waproll_std10_down25_std\n",
      "waproll_std10_down25_skew\n",
      "waproll_std10_down25_autocorr\n",
      "wapcandlef1_down25\n",
      "wapcandlef2_down25\n",
      "wapcandlef3_down25\n",
      "wapcandlef41_down25\n",
      "wapcandlef42_down25\n",
      "up75num\n",
      "waproll_std5_up75_mean\n",
      "waproll_std5_up75_std\n",
      "waproll_std5_up75_skew\n",
      "waproll_std5_up75_autocorr\n",
      "waproll_std10_up75_mean\n",
      "waproll_std10_up75_std\n",
      "waproll_std10_up75_skew\n",
      "waproll_std10_up75_autocorr\n",
      "wapcandlef1_up75\n",
      "wapcandlef2_up75\n",
      "wapcandlef3_up75\n",
      "wapcandlef41_up75\n",
      "wapcandlef42_up75\n",
      "down75num\n",
      "waproll_std5_down75_mean\n",
      "waproll_std5_down75_std\n",
      "waproll_std5_down75_skew\n",
      "waproll_std5_down75_autocorr\n",
      "waproll_std10_down75_mean\n",
      "waproll_std10_down75_std\n",
      "waproll_std10_down75_skew\n",
      "waproll_std10_down75_autocorr\n",
      "wapcandlef1_down75\n",
      "wapcandlef2_down75\n",
      "wapcandlef3_down75\n",
      "wapcandlef41_down75\n",
      "wapcandlef42_down75\n",
      "stock_id\n",
      "book_wap1<lambda>\n",
      "book_wap2<lambda>\n",
      "book_wap_mean<lambda>\n",
      "book_wap_diff<lambda>\n",
      "book_price_spread<lambda>\n",
      "book_bid_spread<lambda>\n",
      "book_ask_spread<lambda>\n",
      "book_total_volume<lambda>\n",
      "book_volume_imbalance<lambda>\n",
      "pricesum\n",
      "pricemean\n",
      "pricestd\n",
      "pricemax\n",
      "pricemin\n",
      "sizesum\n",
      "sizemean\n",
      "sizestd\n",
      "sizemax\n",
      "sizemin\n",
      "order_countsum\n",
      "order_countmean\n",
      "order_countstd\n",
      "order_countmax\n",
      "order_countmin\n",
      "seconds_in_bucketsum\n",
      "seconds_in_bucketmean\n",
      "seconds_in_bucketstd\n",
      "seconds_in_bucketmax\n",
      "seconds_in_bucketmin\n"
     ]
    }
   ],
   "source": [
    "for i in temp.columns:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
