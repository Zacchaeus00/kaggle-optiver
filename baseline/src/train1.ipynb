{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from imp import reload\n",
    "import warnings\n",
    "from utils import util\n",
    "from sklearn import model_selection\n",
    "reload(util)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/20210805.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-aca002b9a932>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/20210805.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/20210805.csv'"
     ]
    }
   ],
   "source": [
    "df_all = pd.read_csv('../data/20210805.csv',index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_stock = df_all[['stock_id']+df_all.columns[df_all.columns.str.endswith('_stock')].tolist()]\n",
    "df_all_stock.drop_duplicates().to_pickle('../data/20210805.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators': 10000,\n",
    "    'objective': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'max_depth': -1,\n",
    "    'learning_rate': 0.01,\n",
    "    'subsample': 0.72,\n",
    "    'subsample_freq': 4,\n",
    "    'feature_fraction': 0.8,\n",
    "    'lambda_l1': 1,\n",
    "    'lambda_l2': 1,\n",
    "    'seed': 66,\n",
    "    'early_stopping_rounds': 300,\n",
    "    'verbose': -1\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_id</th>\n",
       "      <th>stock_id</th>\n",
       "      <th>book_wap1_lambda_</th>\n",
       "      <th>book_wap2_lambda_</th>\n",
       "      <th>book_wap_mean_lambda_</th>\n",
       "      <th>book_wap_diff_lambda_</th>\n",
       "      <th>book_price_spread_lambda_</th>\n",
       "      <th>book_bid_spread_lambda_</th>\n",
       "      <th>book_ask_spread_lambda_</th>\n",
       "      <th>book_total_volume_lambda_</th>\n",
       "      <th>...</th>\n",
       "      <th>__book_wap2_lambda_____min___stock</th>\n",
       "      <th>__book_wap_mean_lambda_____mean___stock</th>\n",
       "      <th>__book_wap_mean_lambda_____std___stock</th>\n",
       "      <th>__book_wap_mean_lambda_____max___stock</th>\n",
       "      <th>__book_wap_mean_lambda_____min___stock</th>\n",
       "      <th>__book_wap_diff_lambda_____mean___stock</th>\n",
       "      <th>__book_wap_diff_lambda_____std___stock</th>\n",
       "      <th>__book_wap_diff_lambda_____max___stock</th>\n",
       "      <th>__book_wap_diff_lambda_____min___stock</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>0.004091</td>\n",
       "      <td>0.005673</td>\n",
       "      <td>0.003606</td>\n",
       "      <td>13.325268</td>\n",
       "      <td>5.686267</td>\n",
       "      <td>20.232372</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.458107</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001142</td>\n",
       "      <td>0.004135</td>\n",
       "      <td>0.003384</td>\n",
       "      <td>0.03336</td>\n",
       "      <td>0.000732</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.70394</td>\n",
       "      <td>0.004615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>0.002155</td>\n",
       "      <td>0.003741</td>\n",
       "      <td>0.002219</td>\n",
       "      <td>9.955957</td>\n",
       "      <td>4.559320</td>\n",
       "      <td>13.929663</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.368241</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001142</td>\n",
       "      <td>0.004135</td>\n",
       "      <td>0.003384</td>\n",
       "      <td>0.03336</td>\n",
       "      <td>0.000732</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.70394</td>\n",
       "      <td>0.002474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>0.002566</td>\n",
       "      <td>0.003324</td>\n",
       "      <td>0.002384</td>\n",
       "      <td>12.558522</td>\n",
       "      <td>5.333334</td>\n",
       "      <td>11.342268</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.852701</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001142</td>\n",
       "      <td>0.004135</td>\n",
       "      <td>0.003384</td>\n",
       "      <td>0.03336</td>\n",
       "      <td>0.000732</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.70394</td>\n",
       "      <td>0.002831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>17</td>\n",
       "      <td>0.002221</td>\n",
       "      <td>0.003017</td>\n",
       "      <td>0.001767</td>\n",
       "      <td>13.153812</td>\n",
       "      <td>3.579197</td>\n",
       "      <td>10.080665</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.123702</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001142</td>\n",
       "      <td>0.004135</td>\n",
       "      <td>0.003384</td>\n",
       "      <td>0.03336</td>\n",
       "      <td>0.000732</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.70394</td>\n",
       "      <td>0.002201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62</td>\n",
       "      <td>17</td>\n",
       "      <td>0.002155</td>\n",
       "      <td>0.003315</td>\n",
       "      <td>0.002078</td>\n",
       "      <td>8.227389</td>\n",
       "      <td>4.508758</td>\n",
       "      <td>10.357522</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.194404</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001142</td>\n",
       "      <td>0.004135</td>\n",
       "      <td>0.003384</td>\n",
       "      <td>0.03336</td>\n",
       "      <td>0.000732</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.70394</td>\n",
       "      <td>0.002090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   time_id  stock_id  book_wap1_lambda_  book_wap2_lambda_  \\\n",
       "0        5        17           0.004091           0.005673   \n",
       "1       11        17           0.002155           0.003741   \n",
       "2       16        17           0.002566           0.003324   \n",
       "3       31        17           0.002221           0.003017   \n",
       "4       62        17           0.002155           0.003315   \n",
       "\n",
       "   book_wap_mean_lambda_  book_wap_diff_lambda_  book_price_spread_lambda_  \\\n",
       "0               0.003606              13.325268                   5.686267   \n",
       "1               0.002219               9.955957                   4.559320   \n",
       "2               0.002384              12.558522                   5.333334   \n",
       "3               0.001767              13.153812                   3.579197   \n",
       "4               0.002078               8.227389                   4.508758   \n",
       "\n",
       "   book_bid_spread_lambda_  book_ask_spread_lambda_  \\\n",
       "0                20.232372                      0.0   \n",
       "1                13.929663                      0.0   \n",
       "2                11.342268                      0.0   \n",
       "3                10.080665                      0.0   \n",
       "4                10.357522                      0.0   \n",
       "\n",
       "   book_total_volume_lambda_  ...  __book_wap2_lambda_____min___stock  \\\n",
       "0                   6.458107  ...                            0.001142   \n",
       "1                   6.368241  ...                            0.001142   \n",
       "2                   4.852701  ...                            0.001142   \n",
       "3                   6.123702  ...                            0.001142   \n",
       "4                   7.194404  ...                            0.001142   \n",
       "\n",
       "   __book_wap_mean_lambda_____mean___stock  \\\n",
       "0                                 0.004135   \n",
       "1                                 0.004135   \n",
       "2                                 0.004135   \n",
       "3                                 0.004135   \n",
       "4                                 0.004135   \n",
       "\n",
       "   __book_wap_mean_lambda_____std___stock  \\\n",
       "0                                0.003384   \n",
       "1                                0.003384   \n",
       "2                                0.003384   \n",
       "3                                0.003384   \n",
       "4                                0.003384   \n",
       "\n",
       "   __book_wap_mean_lambda_____max___stock  \\\n",
       "0                                 0.03336   \n",
       "1                                 0.03336   \n",
       "2                                 0.03336   \n",
       "3                                 0.03336   \n",
       "4                                 0.03336   \n",
       "\n",
       "   __book_wap_mean_lambda_____min___stock  \\\n",
       "0                                0.000732   \n",
       "1                                0.000732   \n",
       "2                                0.000732   \n",
       "3                                0.000732   \n",
       "4                                0.000732   \n",
       "\n",
       "   __book_wap_diff_lambda_____mean___stock  \\\n",
       "0                                      NaN   \n",
       "1                                      NaN   \n",
       "2                                      NaN   \n",
       "3                                      NaN   \n",
       "4                                      NaN   \n",
       "\n",
       "   __book_wap_diff_lambda_____std___stock  \\\n",
       "0                                     NaN   \n",
       "1                                     NaN   \n",
       "2                                     NaN   \n",
       "3                                     NaN   \n",
       "4                                     NaN   \n",
       "\n",
       "   __book_wap_diff_lambda_____max___stock  \\\n",
       "0                                     NaN   \n",
       "1                                     NaN   \n",
       "2                                     NaN   \n",
       "3                                     NaN   \n",
       "4                                     NaN   \n",
       "\n",
       "   __book_wap_diff_lambda_____min___stock    target  \n",
       "0                                 4.70394  0.004615  \n",
       "1                                 4.70394  0.002474  \n",
       "2                                 4.70394  0.002831  \n",
       "3                                 4.70394  0.002201  \n",
       "4                                 4.70394  0.002090  \n",
       "\n",
       "[5 rows x 64 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_all[df_all.columns.difference(['target'])]\n",
    "y_train = df_all['target']\n",
    "features = df_all[df_all.columns.difference(['time_id','target'])].columns.tolist()\n",
    "pd.DataFrame(features).to_pickle('../data/features_name.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 300 rounds\n",
      "[250]\ttraining's rmse: 0.000507826\ttraining's RMSPE: 0.235205\tvalid_1's rmse: 0.000515514\tvalid_1's RMSPE: 0.236963\n",
      "[500]\ttraining's rmse: 0.000478906\ttraining's RMSPE: 0.221811\tvalid_1's rmse: 0.000488429\tvalid_1's RMSPE: 0.224513\n",
      "[750]\ttraining's rmse: 0.000467661\ttraining's RMSPE: 0.216602\tvalid_1's rmse: 0.00048044\tvalid_1's RMSPE: 0.220841\n",
      "[1000]\ttraining's rmse: 0.000459781\ttraining's RMSPE: 0.212953\tvalid_1's rmse: 0.000474885\tvalid_1's RMSPE: 0.218287\n",
      "[1250]\ttraining's rmse: 0.000453479\ttraining's RMSPE: 0.210034\tvalid_1's rmse: 0.000470896\tvalid_1's RMSPE: 0.216453\n",
      "[1500]\ttraining's rmse: 0.000448041\ttraining's RMSPE: 0.207515\tvalid_1's rmse: 0.000467178\tvalid_1's RMSPE: 0.214744\n",
      "[1750]\ttraining's rmse: 0.000443842\ttraining's RMSPE: 0.205571\tvalid_1's rmse: 0.000464631\tvalid_1's RMSPE: 0.213573\n",
      "[2000]\ttraining's rmse: 0.000439618\ttraining's RMSPE: 0.203614\tvalid_1's rmse: 0.00046198\tvalid_1's RMSPE: 0.212355\n",
      "[2250]\ttraining's rmse: 0.000436063\ttraining's RMSPE: 0.201967\tvalid_1's rmse: 0.000460287\tvalid_1's RMSPE: 0.211577\n",
      "[2500]\ttraining's rmse: 0.000433147\ttraining's RMSPE: 0.200617\tvalid_1's rmse: 0.000458583\tvalid_1's RMSPE: 0.210793\n",
      "[2750]\ttraining's rmse: 0.000430029\ttraining's RMSPE: 0.199173\tvalid_1's rmse: 0.00045657\tvalid_1's RMSPE: 0.209868\n",
      "[3000]\ttraining's rmse: 0.00042725\ttraining's RMSPE: 0.197886\tvalid_1's rmse: 0.000455205\tvalid_1's RMSPE: 0.209241\n",
      "[3250]\ttraining's rmse: 0.000424696\ttraining's RMSPE: 0.196703\tvalid_1's rmse: 0.000453783\tvalid_1's RMSPE: 0.208587\n",
      "[3500]\ttraining's rmse: 0.000422328\ttraining's RMSPE: 0.195606\tvalid_1's rmse: 0.000452375\tvalid_1's RMSPE: 0.20794\n",
      "[3750]\ttraining's rmse: 0.000420058\ttraining's RMSPE: 0.194554\tvalid_1's rmse: 0.000451312\tvalid_1's RMSPE: 0.207451\n",
      "[4000]\ttraining's rmse: 0.000417751\ttraining's RMSPE: 0.193486\tvalid_1's rmse: 0.000450482\tvalid_1's RMSPE: 0.20707\n",
      "[4250]\ttraining's rmse: 0.000415692\ttraining's RMSPE: 0.192532\tvalid_1's rmse: 0.000449731\tvalid_1's RMSPE: 0.206725\n",
      "[4500]\ttraining's rmse: 0.000413519\ttraining's RMSPE: 0.191526\tvalid_1's rmse: 0.000448757\tvalid_1's RMSPE: 0.206277\n",
      "[4750]\ttraining's rmse: 0.000411488\ttraining's RMSPE: 0.190586\tvalid_1's rmse: 0.000447875\tvalid_1's RMSPE: 0.205871\n",
      "[5000]\ttraining's rmse: 0.000409571\ttraining's RMSPE: 0.189697\tvalid_1's rmse: 0.000447171\tvalid_1's RMSPE: 0.205548\n",
      "[5250]\ttraining's rmse: 0.000407682\ttraining's RMSPE: 0.188823\tvalid_1's rmse: 0.000446522\tvalid_1's RMSPE: 0.205249\n",
      "[5500]\ttraining's rmse: 0.000405937\ttraining's RMSPE: 0.188014\tvalid_1's rmse: 0.000445734\tvalid_1's RMSPE: 0.204887\n",
      "[5750]\ttraining's rmse: 0.000404091\ttraining's RMSPE: 0.18716\tvalid_1's rmse: 0.000444773\tvalid_1's RMSPE: 0.204445\n",
      "[6000]\ttraining's rmse: 0.000402362\ttraining's RMSPE: 0.186358\tvalid_1's rmse: 0.000444012\tvalid_1's RMSPE: 0.204096\n",
      "[6250]\ttraining's rmse: 0.000400723\ttraining's RMSPE: 0.1856\tvalid_1's rmse: 0.000443369\tvalid_1's RMSPE: 0.2038\n",
      "[6500]\ttraining's rmse: 0.000399071\ttraining's RMSPE: 0.184834\tvalid_1's rmse: 0.00044258\tvalid_1's RMSPE: 0.203438\n",
      "[6750]\ttraining's rmse: 0.000397518\ttraining's RMSPE: 0.184115\tvalid_1's rmse: 0.000441989\tvalid_1's RMSPE: 0.203166\n",
      "[7000]\ttraining's rmse: 0.000395967\ttraining's RMSPE: 0.183397\tvalid_1's rmse: 0.00044144\tvalid_1's RMSPE: 0.202913\n",
      "[7250]\ttraining's rmse: 0.00039459\ttraining's RMSPE: 0.182759\tvalid_1's rmse: 0.000440968\tvalid_1's RMSPE: 0.202697\n",
      "[7500]\ttraining's rmse: 0.000393188\ttraining's RMSPE: 0.18211\tvalid_1's rmse: 0.000440366\tvalid_1's RMSPE: 0.20242\n",
      "[7750]\ttraining's rmse: 0.000391785\ttraining's RMSPE: 0.18146\tvalid_1's rmse: 0.000439694\tvalid_1's RMSPE: 0.202111\n",
      "[8000]\ttraining's rmse: 0.000390413\ttraining's RMSPE: 0.180824\tvalid_1's rmse: 0.000439065\tvalid_1's RMSPE: 0.201822\n",
      "[8250]\ttraining's rmse: 0.000388991\ttraining's RMSPE: 0.180166\tvalid_1's rmse: 0.000438418\tvalid_1's RMSPE: 0.201524\n",
      "[8500]\ttraining's rmse: 0.000387578\ttraining's RMSPE: 0.179511\tvalid_1's rmse: 0.000437736\tvalid_1's RMSPE: 0.201211\n",
      "[8750]\ttraining's rmse: 0.000386363\ttraining's RMSPE: 0.178949\tvalid_1's rmse: 0.000437238\tvalid_1's RMSPE: 0.200982\n",
      "[9000]\ttraining's rmse: 0.000385088\ttraining's RMSPE: 0.178358\tvalid_1's rmse: 0.000436903\tvalid_1's RMSPE: 0.200828\n",
      "[9250]\ttraining's rmse: 0.000383774\ttraining's RMSPE: 0.177749\tvalid_1's rmse: 0.00043647\tvalid_1's RMSPE: 0.200629\n",
      "[9500]\ttraining's rmse: 0.000382568\ttraining's RMSPE: 0.177191\tvalid_1's rmse: 0.000436212\tvalid_1's RMSPE: 0.20051\n",
      "[9750]\ttraining's rmse: 0.000381307\ttraining's RMSPE: 0.176607\tvalid_1's rmse: 0.000435766\tvalid_1's RMSPE: 0.200305\n",
      "[10000]\ttraining's rmse: 0.000380207\ttraining's RMSPE: 0.176097\tvalid_1's rmse: 0.000435534\tvalid_1's RMSPE: 0.200199\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's rmse: 0.000380207\ttraining's RMSPE: 0.176097\tvalid_1's rmse: 0.000435534\tvalid_1's RMSPE: 0.200199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [09:51, 591.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved \n",
      "==================================\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[250]\ttraining's rmse: 0.000507684\ttraining's RMSPE: 0.235024\tvalid_1's rmse: 0.000513837\tvalid_1's RMSPE: 0.23725\n",
      "[500]\ttraining's rmse: 0.000478837\ttraining's RMSPE: 0.22167\tvalid_1's rmse: 0.000489496\tvalid_1's RMSPE: 0.226011\n",
      "[750]\ttraining's rmse: 0.000468125\ttraining's RMSPE: 0.216711\tvalid_1's rmse: 0.000481729\tvalid_1's RMSPE: 0.222425\n",
      "[1000]\ttraining's rmse: 0.000460164\ttraining's RMSPE: 0.213025\tvalid_1's rmse: 0.000476365\tvalid_1's RMSPE: 0.219949\n",
      "[1250]\ttraining's rmse: 0.000453717\ttraining's RMSPE: 0.210041\tvalid_1's rmse: 0.000471962\tvalid_1's RMSPE: 0.217916\n",
      "[1500]\ttraining's rmse: 0.000448178\ttraining's RMSPE: 0.207476\tvalid_1's rmse: 0.000468486\tvalid_1's RMSPE: 0.216311\n",
      "[1750]\ttraining's rmse: 0.000443674\ttraining's RMSPE: 0.205391\tvalid_1's rmse: 0.000465997\tvalid_1's RMSPE: 0.215162\n",
      "[2000]\ttraining's rmse: 0.000439702\ttraining's RMSPE: 0.203553\tvalid_1's rmse: 0.000463847\tvalid_1's RMSPE: 0.214169\n",
      "[2250]\ttraining's rmse: 0.000436106\ttraining's RMSPE: 0.201888\tvalid_1's rmse: 0.00046206\tvalid_1's RMSPE: 0.213344\n",
      "[2500]\ttraining's rmse: 0.000432924\ttraining's RMSPE: 0.200415\tvalid_1's rmse: 0.000460516\tvalid_1's RMSPE: 0.212631\n",
      "[2750]\ttraining's rmse: 0.00042978\ttraining's RMSPE: 0.198959\tvalid_1's rmse: 0.000458798\tvalid_1's RMSPE: 0.211837\n",
      "[3000]\ttraining's rmse: 0.000426871\ttraining's RMSPE: 0.197613\tvalid_1's rmse: 0.000457293\tvalid_1's RMSPE: 0.211142\n",
      "[3250]\ttraining's rmse: 0.000424414\ttraining's RMSPE: 0.196475\tvalid_1's rmse: 0.000456339\tvalid_1's RMSPE: 0.210702\n",
      "[3500]\ttraining's rmse: 0.000421865\ttraining's RMSPE: 0.195296\tvalid_1's rmse: 0.00045513\tvalid_1's RMSPE: 0.210144\n",
      "[3750]\ttraining's rmse: 0.000419613\ttraining's RMSPE: 0.194253\tvalid_1's rmse: 0.000454261\tvalid_1's RMSPE: 0.209742\n",
      "[4000]\ttraining's rmse: 0.000417421\ttraining's RMSPE: 0.193238\tvalid_1's rmse: 0.000453391\tvalid_1's RMSPE: 0.209341\n",
      "[4250]\ttraining's rmse: 0.000415264\ttraining's RMSPE: 0.19224\tvalid_1's rmse: 0.000452474\tvalid_1's RMSPE: 0.208917\n",
      "[4500]\ttraining's rmse: 0.00041325\ttraining's RMSPE: 0.191307\tvalid_1's rmse: 0.000451801\tvalid_1's RMSPE: 0.208607\n",
      "[4750]\ttraining's rmse: 0.000411201\ttraining's RMSPE: 0.190359\tvalid_1's rmse: 0.000451072\tvalid_1's RMSPE: 0.20827\n",
      "[5000]\ttraining's rmse: 0.000409244\ttraining's RMSPE: 0.189453\tvalid_1's rmse: 0.000450218\tvalid_1's RMSPE: 0.207876\n",
      "[5250]\ttraining's rmse: 0.000407456\ttraining's RMSPE: 0.188625\tvalid_1's rmse: 0.000449765\tvalid_1's RMSPE: 0.207667\n",
      "[5500]\ttraining's rmse: 0.000405682\ttraining's RMSPE: 0.187804\tvalid_1's rmse: 0.000449127\tvalid_1's RMSPE: 0.207372\n",
      "[5750]\ttraining's rmse: 0.000403836\ttraining's RMSPE: 0.186949\tvalid_1's rmse: 0.000448375\tvalid_1's RMSPE: 0.207025\n",
      "[6000]\ttraining's rmse: 0.000402085\ttraining's RMSPE: 0.186139\tvalid_1's rmse: 0.000447688\tvalid_1's RMSPE: 0.206707\n",
      "[6250]\ttraining's rmse: 0.000400435\ttraining's RMSPE: 0.185375\tvalid_1's rmse: 0.000447172\tvalid_1's RMSPE: 0.20647\n",
      "[6500]\ttraining's rmse: 0.000398893\ttraining's RMSPE: 0.184661\tvalid_1's rmse: 0.000446768\tvalid_1's RMSPE: 0.206283\n",
      "[6750]\ttraining's rmse: 0.000397378\ttraining's RMSPE: 0.18396\tvalid_1's rmse: 0.000446471\tvalid_1's RMSPE: 0.206146\n",
      "[7000]\ttraining's rmse: 0.000395874\ttraining's RMSPE: 0.183263\tvalid_1's rmse: 0.000446063\tvalid_1's RMSPE: 0.205957\n",
      "[7250]\ttraining's rmse: 0.000394384\ttraining's RMSPE: 0.182573\tvalid_1's rmse: 0.000445644\tvalid_1's RMSPE: 0.205764\n",
      "[7500]\ttraining's rmse: 0.00039283\ttraining's RMSPE: 0.181854\tvalid_1's rmse: 0.000445086\tvalid_1's RMSPE: 0.205506\n",
      "[7750]\ttraining's rmse: 0.000391459\ttraining's RMSPE: 0.18122\tvalid_1's rmse: 0.000444792\tvalid_1's RMSPE: 0.205371\n",
      "[8000]\ttraining's rmse: 0.000390192\ttraining's RMSPE: 0.180633\tvalid_1's rmse: 0.000444481\tvalid_1's RMSPE: 0.205227\n",
      "[8250]\ttraining's rmse: 0.000388902\ttraining's RMSPE: 0.180036\tvalid_1's rmse: 0.000444109\tvalid_1's RMSPE: 0.205055\n",
      "[8500]\ttraining's rmse: 0.000387588\ttraining's RMSPE: 0.179427\tvalid_1's rmse: 0.000443846\tvalid_1's RMSPE: 0.204934\n",
      "[8750]\ttraining's rmse: 0.000386299\ttraining's RMSPE: 0.178831\tvalid_1's rmse: 0.000443574\tvalid_1's RMSPE: 0.204808\n",
      "[9000]\ttraining's rmse: 0.000385051\ttraining's RMSPE: 0.178253\tvalid_1's rmse: 0.000443315\tvalid_1's RMSPE: 0.204688\n",
      "[9250]\ttraining's rmse: 0.000383796\ttraining's RMSPE: 0.177672\tvalid_1's rmse: 0.000442949\tvalid_1's RMSPE: 0.20452\n",
      "[9500]\ttraining's rmse: 0.000382635\ttraining's RMSPE: 0.177135\tvalid_1's rmse: 0.00044276\tvalid_1's RMSPE: 0.204432\n",
      "[9750]\ttraining's rmse: 0.000381428\ttraining's RMSPE: 0.176576\tvalid_1's rmse: 0.000442501\tvalid_1's RMSPE: 0.204313\n",
      "[10000]\ttraining's rmse: 0.000380219\ttraining's RMSPE: 0.176016\tvalid_1's rmse: 0.00044233\tvalid_1's RMSPE: 0.204234\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's rmse: 0.000380219\ttraining's RMSPE: 0.176016\tvalid_1's rmse: 0.00044233\tvalid_1's RMSPE: 0.204234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [19:48, 594.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved \n",
      "==================================\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[250]\ttraining's rmse: 0.000508266\ttraining's RMSPE: 0.235263\tvalid_1's rmse: 0.00050729\tvalid_1's RMSPE: 0.234502\n",
      "[500]\ttraining's rmse: 0.000479514\ttraining's RMSPE: 0.221954\tvalid_1's rmse: 0.000481408\tvalid_1's RMSPE: 0.222537\n",
      "[750]\ttraining's rmse: 0.000468442\ttraining's RMSPE: 0.216829\tvalid_1's rmse: 0.000472586\tvalid_1's RMSPE: 0.218459\n",
      "[1000]\ttraining's rmse: 0.000460471\ttraining's RMSPE: 0.21314\tvalid_1's rmse: 0.000466658\tvalid_1's RMSPE: 0.215719\n",
      "[1250]\ttraining's rmse: 0.000453801\ttraining's RMSPE: 0.210053\tvalid_1's rmse: 0.000461649\tvalid_1's RMSPE: 0.213403\n",
      "[1500]\ttraining's rmse: 0.000448216\ttraining's RMSPE: 0.207467\tvalid_1's rmse: 0.000457859\tvalid_1's RMSPE: 0.211651\n",
      "[1750]\ttraining's rmse: 0.000443702\ttraining's RMSPE: 0.205378\tvalid_1's rmse: 0.00045506\tvalid_1's RMSPE: 0.210358\n",
      "[2000]\ttraining's rmse: 0.000439443\ttraining's RMSPE: 0.203407\tvalid_1's rmse: 0.000452402\tvalid_1's RMSPE: 0.209129\n",
      "[2250]\ttraining's rmse: 0.000436095\ttraining's RMSPE: 0.201857\tvalid_1's rmse: 0.000450751\tvalid_1's RMSPE: 0.208366\n",
      "[2500]\ttraining's rmse: 0.000432854\ttraining's RMSPE: 0.200357\tvalid_1's rmse: 0.000449008\tvalid_1's RMSPE: 0.20756\n",
      "[2750]\ttraining's rmse: 0.000429917\ttraining's RMSPE: 0.198997\tvalid_1's rmse: 0.00044754\tvalid_1's RMSPE: 0.206881\n",
      "[3000]\ttraining's rmse: 0.000427065\ttraining's RMSPE: 0.197677\tvalid_1's rmse: 0.000446132\tvalid_1's RMSPE: 0.20623\n",
      "[3250]\ttraining's rmse: 0.000424546\ttraining's RMSPE: 0.196511\tvalid_1's rmse: 0.000445\tvalid_1's RMSPE: 0.205707\n",
      "[3500]\ttraining's rmse: 0.000422093\ttraining's RMSPE: 0.195376\tvalid_1's rmse: 0.00044384\tvalid_1's RMSPE: 0.205171\n",
      "[3750]\ttraining's rmse: 0.000419949\ttraining's RMSPE: 0.194383\tvalid_1's rmse: 0.000443117\tvalid_1's RMSPE: 0.204837\n",
      "[4000]\ttraining's rmse: 0.000417651\ttraining's RMSPE: 0.19332\tvalid_1's rmse: 0.000442072\tvalid_1's RMSPE: 0.204354\n",
      "[4250]\ttraining's rmse: 0.00041568\ttraining's RMSPE: 0.192407\tvalid_1's rmse: 0.000441461\tvalid_1's RMSPE: 0.204071\n",
      "[4500]\ttraining's rmse: 0.00041361\ttraining's RMSPE: 0.191449\tvalid_1's rmse: 0.0004405\tvalid_1's RMSPE: 0.203627\n",
      "[4750]\ttraining's rmse: 0.000411587\ttraining's RMSPE: 0.190513\tvalid_1's rmse: 0.000439606\tvalid_1's RMSPE: 0.203214\n",
      "[5000]\ttraining's rmse: 0.00040975\ttraining's RMSPE: 0.189662\tvalid_1's rmse: 0.000438894\tvalid_1's RMSPE: 0.202885\n",
      "[5250]\ttraining's rmse: 0.000407959\ttraining's RMSPE: 0.188834\tvalid_1's rmse: 0.000438279\tvalid_1's RMSPE: 0.2026\n",
      "[5500]\ttraining's rmse: 0.000406178\ttraining's RMSPE: 0.188009\tvalid_1's rmse: 0.000437558\tvalid_1's RMSPE: 0.202267\n",
      "[5750]\ttraining's rmse: 0.000404247\ttraining's RMSPE: 0.187115\tvalid_1's rmse: 0.000436827\tvalid_1's RMSPE: 0.201929\n",
      "[6000]\ttraining's rmse: 0.000402536\ttraining's RMSPE: 0.186323\tvalid_1's rmse: 0.000436104\tvalid_1's RMSPE: 0.201595\n",
      "[6250]\ttraining's rmse: 0.000400685\ttraining's RMSPE: 0.185466\tvalid_1's rmse: 0.000435248\tvalid_1's RMSPE: 0.201199\n",
      "[6500]\ttraining's rmse: 0.000399147\ttraining's RMSPE: 0.184755\tvalid_1's rmse: 0.000434896\tvalid_1's RMSPE: 0.201037\n",
      "[6750]\ttraining's rmse: 0.000397581\ttraining's RMSPE: 0.18403\tvalid_1's rmse: 0.000434508\tvalid_1's RMSPE: 0.200857\n",
      "[7000]\ttraining's rmse: 0.000396101\ttraining's RMSPE: 0.183345\tvalid_1's rmse: 0.000434068\tvalid_1's RMSPE: 0.200654\n",
      "[7250]\ttraining's rmse: 0.000394665\ttraining's RMSPE: 0.18268\tvalid_1's rmse: 0.000433674\tvalid_1's RMSPE: 0.200472\n",
      "[7500]\ttraining's rmse: 0.000393206\ttraining's RMSPE: 0.182005\tvalid_1's rmse: 0.000433155\tvalid_1's RMSPE: 0.200232\n",
      "[7750]\ttraining's rmse: 0.000391904\ttraining's RMSPE: 0.181402\tvalid_1's rmse: 0.000432748\tvalid_1's RMSPE: 0.200043\n",
      "[8000]\ttraining's rmse: 0.000390581\ttraining's RMSPE: 0.18079\tvalid_1's rmse: 0.000432407\tvalid_1's RMSPE: 0.199886\n",
      "[8250]\ttraining's rmse: 0.000389272\ttraining's RMSPE: 0.180184\tvalid_1's rmse: 0.000431992\tvalid_1's RMSPE: 0.199694\n",
      "[8500]\ttraining's rmse: 0.000387952\ttraining's RMSPE: 0.179573\tvalid_1's rmse: 0.000431573\tvalid_1's RMSPE: 0.199501\n",
      "[8750]\ttraining's rmse: 0.000386633\ttraining's RMSPE: 0.178962\tvalid_1's rmse: 0.000431116\tvalid_1's RMSPE: 0.199289\n",
      "[9000]\ttraining's rmse: 0.00038537\ttraining's RMSPE: 0.178378\tvalid_1's rmse: 0.000430829\tvalid_1's RMSPE: 0.199156\n",
      "[9250]\ttraining's rmse: 0.000384138\ttraining's RMSPE: 0.177807\tvalid_1's rmse: 0.000430515\tvalid_1's RMSPE: 0.199011\n",
      "[9500]\ttraining's rmse: 0.000382936\ttraining's RMSPE: 0.177251\tvalid_1's rmse: 0.000430185\tvalid_1's RMSPE: 0.198859\n",
      "[9750]\ttraining's rmse: 0.000381739\ttraining's RMSPE: 0.176697\tvalid_1's rmse: 0.000429745\tvalid_1's RMSPE: 0.198655\n",
      "[10000]\ttraining's rmse: 0.000380571\ttraining's RMSPE: 0.176156\tvalid_1's rmse: 0.000429384\tvalid_1's RMSPE: 0.198489\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's rmse: 0.000380571\ttraining's RMSPE: 0.176156\tvalid_1's rmse: 0.000429384\tvalid_1's RMSPE: 0.198489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [29:46, 596.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved \n",
      "==================================\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[250]\ttraining's rmse: 0.000507512\ttraining's RMSPE: 0.234895\tvalid_1's rmse: 0.000516762\tvalid_1's RMSPE: 0.239055\n",
      "[500]\ttraining's rmse: 0.000478673\ttraining's RMSPE: 0.221547\tvalid_1's rmse: 0.0004904\tvalid_1's RMSPE: 0.226859\n",
      "[750]\ttraining's rmse: 0.000467786\ttraining's RMSPE: 0.216508\tvalid_1's rmse: 0.000482081\tvalid_1's RMSPE: 0.223011\n",
      "[1000]\ttraining's rmse: 0.000460123\ttraining's RMSPE: 0.212962\tvalid_1's rmse: 0.000476705\tvalid_1's RMSPE: 0.220524\n",
      "[1250]\ttraining's rmse: 0.000453385\ttraining's RMSPE: 0.209843\tvalid_1's rmse: 0.000471622\tvalid_1's RMSPE: 0.218173\n",
      "[1500]\ttraining's rmse: 0.000448177\ttraining's RMSPE: 0.207433\tvalid_1's rmse: 0.000468189\tvalid_1's RMSPE: 0.216584\n",
      "[1750]\ttraining's rmse: 0.000443755\ttraining's RMSPE: 0.205386\tvalid_1's rmse: 0.000465508\tvalid_1's RMSPE: 0.215344\n",
      "[2000]\ttraining's rmse: 0.000439855\ttraining's RMSPE: 0.203581\tvalid_1's rmse: 0.000463148\tvalid_1's RMSPE: 0.214253\n",
      "[2250]\ttraining's rmse: 0.000436147\ttraining's RMSPE: 0.201865\tvalid_1's rmse: 0.000460891\tvalid_1's RMSPE: 0.213209\n",
      "[2500]\ttraining's rmse: 0.000432861\ttraining's RMSPE: 0.200344\tvalid_1's rmse: 0.000459033\tvalid_1's RMSPE: 0.212349\n",
      "[2750]\ttraining's rmse: 0.000429756\ttraining's RMSPE: 0.198907\tvalid_1's rmse: 0.000457447\tvalid_1's RMSPE: 0.211615\n",
      "[3000]\ttraining's rmse: 0.000426841\ttraining's RMSPE: 0.197558\tvalid_1's rmse: 0.000456048\tvalid_1's RMSPE: 0.210968\n",
      "[3250]\ttraining's rmse: 0.000424367\ttraining's RMSPE: 0.196413\tvalid_1's rmse: 0.000455006\tvalid_1's RMSPE: 0.210486\n",
      "[3500]\ttraining's rmse: 0.000421956\ttraining's RMSPE: 0.195297\tvalid_1's rmse: 0.000454057\tvalid_1's RMSPE: 0.210047\n",
      "[3750]\ttraining's rmse: 0.000419754\ttraining's RMSPE: 0.194277\tvalid_1's rmse: 0.000453319\tvalid_1's RMSPE: 0.209706\n",
      "[4000]\ttraining's rmse: 0.000417577\ttraining's RMSPE: 0.19327\tvalid_1's rmse: 0.000452383\tvalid_1's RMSPE: 0.209273\n",
      "[4250]\ttraining's rmse: 0.000415458\ttraining's RMSPE: 0.192289\tvalid_1's rmse: 0.000451647\tvalid_1's RMSPE: 0.208932\n",
      "[4500]\ttraining's rmse: 0.000413287\ttraining's RMSPE: 0.191284\tvalid_1's rmse: 0.000450544\tvalid_1's RMSPE: 0.208422\n",
      "[4750]\ttraining's rmse: 0.000411411\ttraining's RMSPE: 0.190416\tvalid_1's rmse: 0.000450003\tvalid_1's RMSPE: 0.208172\n",
      "[5000]\ttraining's rmse: 0.000409545\ttraining's RMSPE: 0.189552\tvalid_1's rmse: 0.000449568\tvalid_1's RMSPE: 0.207971\n",
      "[5250]\ttraining's rmse: 0.000407647\ttraining's RMSPE: 0.188674\tvalid_1's rmse: 0.000448833\tvalid_1's RMSPE: 0.20763\n",
      "[5500]\ttraining's rmse: 0.000406001\ttraining's RMSPE: 0.187912\tvalid_1's rmse: 0.000448554\tvalid_1's RMSPE: 0.207501\n",
      "[5750]\ttraining's rmse: 0.000404139\ttraining's RMSPE: 0.18705\tvalid_1's rmse: 0.000447885\tvalid_1's RMSPE: 0.207192\n",
      "[6000]\ttraining's rmse: 0.000402406\ttraining's RMSPE: 0.186248\tvalid_1's rmse: 0.000447094\tvalid_1's RMSPE: 0.206826\n",
      "[6250]\ttraining's rmse: 0.000400606\ttraining's RMSPE: 0.185415\tvalid_1's rmse: 0.000446397\tvalid_1's RMSPE: 0.206504\n",
      "[6500]\ttraining's rmse: 0.000398938\ttraining's RMSPE: 0.184643\tvalid_1's rmse: 0.00044585\tvalid_1's RMSPE: 0.20625\n",
      "[6750]\ttraining's rmse: 0.000397214\ttraining's RMSPE: 0.183845\tvalid_1's rmse: 0.000445227\tvalid_1's RMSPE: 0.205963\n",
      "[7000]\ttraining's rmse: 0.000395768\ttraining's RMSPE: 0.183176\tvalid_1's rmse: 0.00044487\tvalid_1's RMSPE: 0.205797\n",
      "[7250]\ttraining's rmse: 0.000394333\ttraining's RMSPE: 0.182511\tvalid_1's rmse: 0.000444399\tvalid_1's RMSPE: 0.205579\n",
      "[7500]\ttraining's rmse: 0.000392934\ttraining's RMSPE: 0.181864\tvalid_1's rmse: 0.0004441\tvalid_1's RMSPE: 0.205441\n",
      "[7750]\ttraining's rmse: 0.000391578\ttraining's RMSPE: 0.181236\tvalid_1's rmse: 0.000443598\tvalid_1's RMSPE: 0.205209\n",
      "[8000]\ttraining's rmse: 0.000390182\ttraining's RMSPE: 0.18059\tvalid_1's rmse: 0.000443077\tvalid_1's RMSPE: 0.204968\n",
      "[8250]\ttraining's rmse: 0.000388879\ttraining's RMSPE: 0.179987\tvalid_1's rmse: 0.00044265\tvalid_1's RMSPE: 0.20477\n",
      "[8500]\ttraining's rmse: 0.000387433\ttraining's RMSPE: 0.179318\tvalid_1's rmse: 0.00044211\tvalid_1's RMSPE: 0.204521\n",
      "[8750]\ttraining's rmse: 0.000386208\ttraining's RMSPE: 0.178751\tvalid_1's rmse: 0.000441914\tvalid_1's RMSPE: 0.20443\n",
      "[9000]\ttraining's rmse: 0.000384853\ttraining's RMSPE: 0.178124\tvalid_1's rmse: 0.000441496\tvalid_1's RMSPE: 0.204237\n",
      "[9250]\ttraining's rmse: 0.000383544\ttraining's RMSPE: 0.177518\tvalid_1's rmse: 0.000441187\tvalid_1's RMSPE: 0.204093\n",
      "[9500]\ttraining's rmse: 0.000382341\ttraining's RMSPE: 0.176961\tvalid_1's rmse: 0.000440866\tvalid_1's RMSPE: 0.203945\n",
      "[9750]\ttraining's rmse: 0.000381116\ttraining's RMSPE: 0.176394\tvalid_1's rmse: 0.000440695\tvalid_1's RMSPE: 0.203866\n",
      "[10000]\ttraining's rmse: 0.00037995\ttraining's RMSPE: 0.175855\tvalid_1's rmse: 0.000440401\tvalid_1's RMSPE: 0.20373\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's rmse: 0.00037995\ttraining's RMSPE: 0.175855\tvalid_1's rmse: 0.000440401\tvalid_1's RMSPE: 0.20373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [39:45, 597.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved \n",
      "==================================\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[250]\ttraining's rmse: 0.000508236\ttraining's RMSPE: 0.235162\tvalid_1's rmse: 0.00052095\tvalid_1's RMSPE: 0.241621\n",
      "[500]\ttraining's rmse: 0.00047908\ttraining's RMSPE: 0.221671\tvalid_1's rmse: 0.000495012\tvalid_1's RMSPE: 0.229591\n",
      "[750]\ttraining's rmse: 0.000468254\ttraining's RMSPE: 0.216662\tvalid_1's rmse: 0.000487721\tvalid_1's RMSPE: 0.22621\n",
      "[1000]\ttraining's rmse: 0.000459842\ttraining's RMSPE: 0.21277\tvalid_1's rmse: 0.000482926\tvalid_1's RMSPE: 0.223986\n",
      "[1250]\ttraining's rmse: 0.000453478\ttraining's RMSPE: 0.209825\tvalid_1's rmse: 0.000479349\tvalid_1's RMSPE: 0.222327\n",
      "[1500]\ttraining's rmse: 0.000448103\ttraining's RMSPE: 0.207338\tvalid_1's rmse: 0.00047713\tvalid_1's RMSPE: 0.221297\n",
      "[1750]\ttraining's rmse: 0.000443531\ttraining's RMSPE: 0.205223\tvalid_1's rmse: 0.000475254\tvalid_1's RMSPE: 0.220427\n",
      "[2000]\ttraining's rmse: 0.000439509\ttraining's RMSPE: 0.203361\tvalid_1's rmse: 0.000472833\tvalid_1's RMSPE: 0.219304\n",
      "[2250]\ttraining's rmse: 0.000435964\ttraining's RMSPE: 0.201722\tvalid_1's rmse: 0.000471637\tvalid_1's RMSPE: 0.21875\n",
      "[2500]\ttraining's rmse: 0.000432675\ttraining's RMSPE: 0.2002\tvalid_1's rmse: 0.000469806\tvalid_1's RMSPE: 0.2179\n",
      "[2750]\ttraining's rmse: 0.000429582\ttraining's RMSPE: 0.198768\tvalid_1's rmse: 0.000468437\tvalid_1's RMSPE: 0.217265\n",
      "[3000]\ttraining's rmse: 0.000426775\ttraining's RMSPE: 0.197469\tvalid_1's rmse: 0.000468102\tvalid_1's RMSPE: 0.21711\n",
      "[3250]\ttraining's rmse: 0.000424405\ttraining's RMSPE: 0.196373\tvalid_1's rmse: 0.000467948\tvalid_1's RMSPE: 0.217038\n",
      "[3500]\ttraining's rmse: 0.000421834\ttraining's RMSPE: 0.195183\tvalid_1's rmse: 0.00046692\tvalid_1's RMSPE: 0.216562\n",
      "[3750]\ttraining's rmse: 0.000419555\ttraining's RMSPE: 0.194129\tvalid_1's rmse: 0.000465882\tvalid_1's RMSPE: 0.216081\n",
      "[4000]\ttraining's rmse: 0.000417315\ttraining's RMSPE: 0.193092\tvalid_1's rmse: 0.000465289\tvalid_1's RMSPE: 0.215805\n",
      "[4250]\ttraining's rmse: 0.000415258\ttraining's RMSPE: 0.192141\tvalid_1's rmse: 0.000464263\tvalid_1's RMSPE: 0.215329\n",
      "[4500]\ttraining's rmse: 0.000413179\ttraining's RMSPE: 0.191179\tvalid_1's rmse: 0.000464247\tvalid_1's RMSPE: 0.215322\n",
      "[4750]\ttraining's rmse: 0.000411129\ttraining's RMSPE: 0.19023\tvalid_1's rmse: 0.000463307\tvalid_1's RMSPE: 0.214886\n",
      "[5000]\ttraining's rmse: 0.000409204\ttraining's RMSPE: 0.189339\tvalid_1's rmse: 0.000462612\tvalid_1's RMSPE: 0.214564\n",
      "[5250]\ttraining's rmse: 0.000407044\ttraining's RMSPE: 0.18834\tvalid_1's rmse: 0.000461274\tvalid_1's RMSPE: 0.213943\n",
      "[5500]\ttraining's rmse: 0.000405247\ttraining's RMSPE: 0.187508\tvalid_1's rmse: 0.000460902\tvalid_1's RMSPE: 0.213771\n",
      "[5750]\ttraining's rmse: 0.000403502\ttraining's RMSPE: 0.186701\tvalid_1's rmse: 0.00046056\tvalid_1's RMSPE: 0.213612\n",
      "[6000]\ttraining's rmse: 0.000401912\ttraining's RMSPE: 0.185965\tvalid_1's rmse: 0.00045976\tvalid_1's RMSPE: 0.213241\n",
      "[6250]\ttraining's rmse: 0.000400195\ttraining's RMSPE: 0.185171\tvalid_1's rmse: 0.000459033\tvalid_1's RMSPE: 0.212904\n",
      "[6500]\ttraining's rmse: 0.000398503\ttraining's RMSPE: 0.184388\tvalid_1's rmse: 0.00045831\tvalid_1's RMSPE: 0.212568\n",
      "[6750]\ttraining's rmse: 0.000396926\ttraining's RMSPE: 0.183659\tvalid_1's rmse: 0.000457918\tvalid_1's RMSPE: 0.212387\n",
      "[7000]\ttraining's rmse: 0.000395517\ttraining's RMSPE: 0.183006\tvalid_1's rmse: 0.000457758\tvalid_1's RMSPE: 0.212312\n",
      "[7250]\ttraining's rmse: 0.000394115\ttraining's RMSPE: 0.182358\tvalid_1's rmse: 0.000457502\tvalid_1's RMSPE: 0.212194\n",
      "[7500]\ttraining's rmse: 0.000392739\ttraining's RMSPE: 0.181721\tvalid_1's rmse: 0.000457983\tvalid_1's RMSPE: 0.212417\n",
      "Early stopping, best iteration is:\n",
      "[7259]\ttraining's rmse: 0.000394066\ttraining's RMSPE: 0.182335\tvalid_1's rmse: 0.000457455\tvalid_1's RMSPE: 0.212172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [47:26, 548.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved \n",
      "==================================\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[250]\ttraining's rmse: 0.000507257\ttraining's RMSPE: 0.234729\tvalid_1's rmse: 0.000515886\tvalid_1's RMSPE: 0.239091\n",
      "[500]\ttraining's rmse: 0.000478406\ttraining's RMSPE: 0.221378\tvalid_1's rmse: 0.000490165\tvalid_1's RMSPE: 0.22717\n",
      "[750]\ttraining's rmse: 0.000467339\ttraining's RMSPE: 0.216257\tvalid_1's rmse: 0.000480954\tvalid_1's RMSPE: 0.222901\n",
      "[1000]\ttraining's rmse: 0.000459079\ttraining's RMSPE: 0.212435\tvalid_1's rmse: 0.000474674\tvalid_1's RMSPE: 0.219991\n",
      "[1250]\ttraining's rmse: 0.000452817\ttraining's RMSPE: 0.209537\tvalid_1's rmse: 0.000470096\tvalid_1's RMSPE: 0.217869\n",
      "[1500]\ttraining's rmse: 0.000447516\ttraining's RMSPE: 0.207084\tvalid_1's rmse: 0.000466423\tvalid_1's RMSPE: 0.216167\n",
      "[1750]\ttraining's rmse: 0.000442636\ttraining's RMSPE: 0.204826\tvalid_1's rmse: 0.000462926\tvalid_1's RMSPE: 0.214546\n",
      "[2000]\ttraining's rmse: 0.000438442\ttraining's RMSPE: 0.202885\tvalid_1's rmse: 0.000460011\tvalid_1's RMSPE: 0.213195\n",
      "[2250]\ttraining's rmse: 0.00043499\ttraining's RMSPE: 0.201288\tvalid_1's rmse: 0.000458066\tvalid_1's RMSPE: 0.212293\n",
      "[2500]\ttraining's rmse: 0.000432004\ttraining's RMSPE: 0.199906\tvalid_1's rmse: 0.000456302\tvalid_1's RMSPE: 0.211476\n",
      "[2750]\ttraining's rmse: 0.00042876\ttraining's RMSPE: 0.198405\tvalid_1's rmse: 0.000454171\tvalid_1's RMSPE: 0.210488\n",
      "[3000]\ttraining's rmse: 0.000425886\ttraining's RMSPE: 0.197075\tvalid_1's rmse: 0.000452535\tvalid_1's RMSPE: 0.20973\n",
      "[3250]\ttraining's rmse: 0.000423484\ttraining's RMSPE: 0.195963\tvalid_1's rmse: 0.000451449\tvalid_1's RMSPE: 0.209227\n",
      "[3500]\ttraining's rmse: 0.000421159\ttraining's RMSPE: 0.194888\tvalid_1's rmse: 0.000450208\tvalid_1's RMSPE: 0.208652\n",
      "[3750]\ttraining's rmse: 0.000418882\ttraining's RMSPE: 0.193834\tvalid_1's rmse: 0.000449142\tvalid_1's RMSPE: 0.208158\n",
      "[4000]\ttraining's rmse: 0.000416832\ttraining's RMSPE: 0.192885\tvalid_1's rmse: 0.000448309\tvalid_1's RMSPE: 0.207772\n",
      "[4250]\ttraining's rmse: 0.000414792\ttraining's RMSPE: 0.191941\tvalid_1's rmse: 0.000447394\tvalid_1's RMSPE: 0.207348\n",
      "[4500]\ttraining's rmse: 0.000412698\ttraining's RMSPE: 0.190972\tvalid_1's rmse: 0.000446475\tvalid_1's RMSPE: 0.206922\n",
      "[4750]\ttraining's rmse: 0.000410771\ttraining's RMSPE: 0.190081\tvalid_1's rmse: 0.000445541\tvalid_1's RMSPE: 0.206489\n",
      "[5000]\ttraining's rmse: 0.000408965\ttraining's RMSPE: 0.189245\tvalid_1's rmse: 0.000444787\tvalid_1's RMSPE: 0.206139\n",
      "[5250]\ttraining's rmse: 0.000407107\ttraining's RMSPE: 0.188385\tvalid_1's rmse: 0.000443937\tvalid_1's RMSPE: 0.205746\n",
      "[5500]\ttraining's rmse: 0.000405373\ttraining's RMSPE: 0.187583\tvalid_1's rmse: 0.000443142\tvalid_1's RMSPE: 0.205377\n",
      "[5750]\ttraining's rmse: 0.000403535\ttraining's RMSPE: 0.186732\tvalid_1's rmse: 0.000442251\tvalid_1's RMSPE: 0.204964\n",
      "[6000]\ttraining's rmse: 0.000401771\ttraining's RMSPE: 0.185916\tvalid_1's rmse: 0.000441417\tvalid_1's RMSPE: 0.204578\n",
      "[6250]\ttraining's rmse: 0.000400138\ttraining's RMSPE: 0.18516\tvalid_1's rmse: 0.000440673\tvalid_1's RMSPE: 0.204233\n",
      "[6500]\ttraining's rmse: 0.000398641\ttraining's RMSPE: 0.184468\tvalid_1's rmse: 0.000440109\tvalid_1's RMSPE: 0.203971\n",
      "[6750]\ttraining's rmse: 0.000397094\ttraining's RMSPE: 0.183752\tvalid_1's rmse: 0.00043948\tvalid_1's RMSPE: 0.20368\n",
      "[7000]\ttraining's rmse: 0.000395623\ttraining's RMSPE: 0.183071\tvalid_1's rmse: 0.000438989\tvalid_1's RMSPE: 0.203452\n",
      "[7250]\ttraining's rmse: 0.000394226\ttraining's RMSPE: 0.182425\tvalid_1's rmse: 0.000438579\tvalid_1's RMSPE: 0.203262\n",
      "[7500]\ttraining's rmse: 0.000392842\ttraining's RMSPE: 0.181784\tvalid_1's rmse: 0.000438062\tvalid_1's RMSPE: 0.203023\n",
      "[7750]\ttraining's rmse: 0.000391461\ttraining's RMSPE: 0.181145\tvalid_1's rmse: 0.000437565\tvalid_1's RMSPE: 0.202792\n",
      "[8000]\ttraining's rmse: 0.000390096\ttraining's RMSPE: 0.180513\tvalid_1's rmse: 0.000437\tvalid_1's RMSPE: 0.20253\n",
      "[8250]\ttraining's rmse: 0.000388759\ttraining's RMSPE: 0.179895\tvalid_1's rmse: 0.000436547\tvalid_1's RMSPE: 0.202321\n",
      "[8500]\ttraining's rmse: 0.000387451\ttraining's RMSPE: 0.179289\tvalid_1's rmse: 0.000436147\tvalid_1's RMSPE: 0.202135\n",
      "[8750]\ttraining's rmse: 0.000386152\ttraining's RMSPE: 0.178688\tvalid_1's rmse: 0.000435762\tvalid_1's RMSPE: 0.201957\n",
      "[9000]\ttraining's rmse: 0.000384862\ttraining's RMSPE: 0.178092\tvalid_1's rmse: 0.000435276\tvalid_1's RMSPE: 0.201732\n",
      "[9250]\ttraining's rmse: 0.000383648\ttraining's RMSPE: 0.17753\tvalid_1's rmse: 0.00043492\tvalid_1's RMSPE: 0.201567\n",
      "[9500]\ttraining's rmse: 0.000382447\ttraining's RMSPE: 0.176974\tvalid_1's rmse: 0.000434592\tvalid_1's RMSPE: 0.201415\n",
      "[9750]\ttraining's rmse: 0.000381147\ttraining's RMSPE: 0.176372\tvalid_1's rmse: 0.000434088\tvalid_1's RMSPE: 0.201181\n",
      "[10000]\ttraining's rmse: 0.000380051\ttraining's RMSPE: 0.175865\tvalid_1's rmse: 0.000433947\tvalid_1's RMSPE: 0.201116\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's rmse: 0.000380051\ttraining's RMSPE: 0.175865\tvalid_1's rmse: 0.000433947\tvalid_1's RMSPE: 0.201116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [57:24, 565.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved \n",
      "==================================\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[250]\ttraining's rmse: 0.000508097\ttraining's RMSPE: 0.235054\tvalid_1's rmse: 0.000512924\tvalid_1's RMSPE: 0.238292\n",
      "[500]\ttraining's rmse: 0.000479254\ttraining's RMSPE: 0.221711\tvalid_1's rmse: 0.000486204\tvalid_1's RMSPE: 0.225879\n",
      "[750]\ttraining's rmse: 0.00046802\ttraining's RMSPE: 0.216514\tvalid_1's rmse: 0.000479244\tvalid_1's RMSPE: 0.222645\n",
      "[1000]\ttraining's rmse: 0.000460068\ttraining's RMSPE: 0.212835\tvalid_1's rmse: 0.000474213\tvalid_1's RMSPE: 0.220308\n",
      "[1250]\ttraining's rmse: 0.000453733\ttraining's RMSPE: 0.209904\tvalid_1's rmse: 0.000470809\tvalid_1's RMSPE: 0.218726\n",
      "[1500]\ttraining's rmse: 0.000448335\ttraining's RMSPE: 0.207407\tvalid_1's rmse: 0.000467509\tvalid_1's RMSPE: 0.217193\n",
      "[1750]\ttraining's rmse: 0.000444225\ttraining's RMSPE: 0.205506\tvalid_1's rmse: 0.000466016\tvalid_1's RMSPE: 0.2165\n",
      "[2000]\ttraining's rmse: 0.000440205\ttraining's RMSPE: 0.203646\tvalid_1's rmse: 0.000464093\tvalid_1's RMSPE: 0.215607\n",
      "[2250]\ttraining's rmse: 0.000436785\ttraining's RMSPE: 0.202064\tvalid_1's rmse: 0.00046247\tvalid_1's RMSPE: 0.214853\n",
      "[2500]\ttraining's rmse: 0.000433532\ttraining's RMSPE: 0.200559\tvalid_1's rmse: 0.000460793\tvalid_1's RMSPE: 0.214073\n",
      "[2750]\ttraining's rmse: 0.00043059\ttraining's RMSPE: 0.199198\tvalid_1's rmse: 0.00045968\tvalid_1's RMSPE: 0.213556\n",
      "[3000]\ttraining's rmse: 0.000427813\ttraining's RMSPE: 0.197913\tvalid_1's rmse: 0.000458867\tvalid_1's RMSPE: 0.213178\n",
      "[3250]\ttraining's rmse: 0.000425399\ttraining's RMSPE: 0.196796\tvalid_1's rmse: 0.000458065\tvalid_1's RMSPE: 0.212806\n",
      "[3500]\ttraining's rmse: 0.00042319\ttraining's RMSPE: 0.195775\tvalid_1's rmse: 0.000456915\tvalid_1's RMSPE: 0.212272\n",
      "[3750]\ttraining's rmse: 0.000420813\ttraining's RMSPE: 0.194675\tvalid_1's rmse: 0.000455711\tvalid_1's RMSPE: 0.211712\n",
      "[4000]\ttraining's rmse: 0.000418345\ttraining's RMSPE: 0.193533\tvalid_1's rmse: 0.000454317\tvalid_1's RMSPE: 0.211065\n",
      "[4250]\ttraining's rmse: 0.000416192\ttraining's RMSPE: 0.192537\tvalid_1's rmse: 0.00045397\tvalid_1's RMSPE: 0.210903\n",
      "[4500]\ttraining's rmse: 0.000414009\ttraining's RMSPE: 0.191527\tvalid_1's rmse: 0.000453378\tvalid_1's RMSPE: 0.210629\n",
      "[4750]\ttraining's rmse: 0.000411918\ttraining's RMSPE: 0.19056\tvalid_1's rmse: 0.000452348\tvalid_1's RMSPE: 0.21015\n",
      "[5000]\ttraining's rmse: 0.000410033\ttraining's RMSPE: 0.189688\tvalid_1's rmse: 0.000451358\tvalid_1's RMSPE: 0.20969\n",
      "[5250]\ttraining's rmse: 0.000408285\ttraining's RMSPE: 0.18888\tvalid_1's rmse: 0.000450812\tvalid_1's RMSPE: 0.209437\n",
      "[5500]\ttraining's rmse: 0.000406612\ttraining's RMSPE: 0.188105\tvalid_1's rmse: 0.000450374\tvalid_1's RMSPE: 0.209233\n",
      "[5750]\ttraining's rmse: 0.000404806\ttraining's RMSPE: 0.18727\tvalid_1's rmse: 0.000449582\tvalid_1's RMSPE: 0.208865\n",
      "[6000]\ttraining's rmse: 0.000403048\ttraining's RMSPE: 0.186456\tvalid_1's rmse: 0.000448797\tvalid_1's RMSPE: 0.2085\n",
      "[6250]\ttraining's rmse: 0.000401461\ttraining's RMSPE: 0.185723\tvalid_1's rmse: 0.000447933\tvalid_1's RMSPE: 0.208099\n",
      "[6500]\ttraining's rmse: 0.000399878\ttraining's RMSPE: 0.18499\tvalid_1's rmse: 0.000447192\tvalid_1's RMSPE: 0.207755\n",
      "[6750]\ttraining's rmse: 0.000398224\ttraining's RMSPE: 0.184225\tvalid_1's rmse: 0.000446482\tvalid_1's RMSPE: 0.207425\n",
      "[7000]\ttraining's rmse: 0.00039678\ttraining's RMSPE: 0.183557\tvalid_1's rmse: 0.000446036\tvalid_1's RMSPE: 0.207217\n",
      "[7250]\ttraining's rmse: 0.000395312\ttraining's RMSPE: 0.182878\tvalid_1's rmse: 0.000445406\tvalid_1's RMSPE: 0.206925\n",
      "[7500]\ttraining's rmse: 0.00039374\ttraining's RMSPE: 0.182151\tvalid_1's rmse: 0.000444931\tvalid_1's RMSPE: 0.206704\n",
      "[7750]\ttraining's rmse: 0.000392318\ttraining's RMSPE: 0.181493\tvalid_1's rmse: 0.0004444\tvalid_1's RMSPE: 0.206458\n",
      "[8000]\ttraining's rmse: 0.000391029\ttraining's RMSPE: 0.180896\tvalid_1's rmse: 0.000443957\tvalid_1's RMSPE: 0.206252\n",
      "[8250]\ttraining's rmse: 0.000389683\ttraining's RMSPE: 0.180274\tvalid_1's rmse: 0.000443515\tvalid_1's RMSPE: 0.206046\n",
      "[8500]\ttraining's rmse: 0.000388312\ttraining's RMSPE: 0.17964\tvalid_1's rmse: 0.000443278\tvalid_1's RMSPE: 0.205936\n",
      "[8750]\ttraining's rmse: 0.000387111\ttraining's RMSPE: 0.179084\tvalid_1's rmse: 0.000442965\tvalid_1's RMSPE: 0.205791\n",
      "[9000]\ttraining's rmse: 0.00038581\ttraining's RMSPE: 0.178482\tvalid_1's rmse: 0.00044252\tvalid_1's RMSPE: 0.205584\n",
      "[9250]\ttraining's rmse: 0.000384624\ttraining's RMSPE: 0.177933\tvalid_1's rmse: 0.000442374\tvalid_1's RMSPE: 0.205516\n",
      "[9500]\ttraining's rmse: 0.000383417\ttraining's RMSPE: 0.177375\tvalid_1's rmse: 0.00044203\tvalid_1's RMSPE: 0.205357\n",
      "[9750]\ttraining's rmse: 0.000382134\ttraining's RMSPE: 0.176782\tvalid_1's rmse: 0.000441697\tvalid_1's RMSPE: 0.205202\n",
      "[10000]\ttraining's rmse: 0.000380981\ttraining's RMSPE: 0.176248\tvalid_1's rmse: 0.000441352\tvalid_1's RMSPE: 0.205042\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's rmse: 0.000380981\ttraining's RMSPE: 0.176248\tvalid_1's rmse: 0.000441352\tvalid_1's RMSPE: 0.205042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [1:07:08, 571.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved \n",
      "==================================\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[250]\ttraining's rmse: 0.000507769\ttraining's RMSPE: 0.235036\tvalid_1's rmse: 0.00052215\tvalid_1's RMSPE: 0.241342\n",
      "[500]\ttraining's rmse: 0.000478966\ttraining's RMSPE: 0.221704\tvalid_1's rmse: 0.000494213\tvalid_1's RMSPE: 0.228429\n",
      "[750]\ttraining's rmse: 0.000468124\ttraining's RMSPE: 0.216685\tvalid_1's rmse: 0.00048564\tvalid_1's RMSPE: 0.224466\n",
      "[1000]\ttraining's rmse: 0.000459998\ttraining's RMSPE: 0.212924\tvalid_1's rmse: 0.000479601\tvalid_1's RMSPE: 0.221675\n",
      "[1250]\ttraining's rmse: 0.000453305\ttraining's RMSPE: 0.209826\tvalid_1's rmse: 0.000475313\tvalid_1's RMSPE: 0.219693\n",
      "[1500]\ttraining's rmse: 0.000447629\ttraining's RMSPE: 0.207199\tvalid_1's rmse: 0.000471837\tvalid_1's RMSPE: 0.218087\n",
      "[1750]\ttraining's rmse: 0.000443199\ttraining's RMSPE: 0.205148\tvalid_1's rmse: 0.00046928\tvalid_1's RMSPE: 0.216904\n",
      "[2000]\ttraining's rmse: 0.000439246\ttraining's RMSPE: 0.203318\tvalid_1's rmse: 0.000466976\tvalid_1's RMSPE: 0.215839\n",
      "[2250]\ttraining's rmse: 0.000435769\ttraining's RMSPE: 0.201709\tvalid_1's rmse: 0.000464949\tvalid_1's RMSPE: 0.214902\n",
      "[2500]\ttraining's rmse: 0.000432567\ttraining's RMSPE: 0.200226\tvalid_1's rmse: 0.000463225\tvalid_1's RMSPE: 0.214106\n",
      "[2750]\ttraining's rmse: 0.000429581\ttraining's RMSPE: 0.198844\tvalid_1's rmse: 0.000461469\tvalid_1's RMSPE: 0.213294\n",
      "[3000]\ttraining's rmse: 0.000426819\ttraining's RMSPE: 0.197566\tvalid_1's rmse: 0.000460041\tvalid_1's RMSPE: 0.212634\n",
      "[3250]\ttraining's rmse: 0.000424283\ttraining's RMSPE: 0.196392\tvalid_1's rmse: 0.00045873\tvalid_1's RMSPE: 0.212028\n",
      "[3500]\ttraining's rmse: 0.000422058\ttraining's RMSPE: 0.195362\tvalid_1's rmse: 0.000457574\tvalid_1's RMSPE: 0.211494\n",
      "[3750]\ttraining's rmse: 0.000419641\ttraining's RMSPE: 0.194243\tvalid_1's rmse: 0.000456145\tvalid_1's RMSPE: 0.210833\n",
      "[4000]\ttraining's rmse: 0.000417394\ttraining's RMSPE: 0.193203\tvalid_1's rmse: 0.00045486\tvalid_1's RMSPE: 0.21024\n",
      "[4250]\ttraining's rmse: 0.000415333\ttraining's RMSPE: 0.192249\tvalid_1's rmse: 0.000453792\tvalid_1's RMSPE: 0.209746\n",
      "[4500]\ttraining's rmse: 0.000413172\ttraining's RMSPE: 0.191249\tvalid_1's rmse: 0.000452672\tvalid_1's RMSPE: 0.209228\n",
      "[4750]\ttraining's rmse: 0.000411044\ttraining's RMSPE: 0.190264\tvalid_1's rmse: 0.000451671\tvalid_1's RMSPE: 0.208765\n",
      "[5000]\ttraining's rmse: 0.00040915\ttraining's RMSPE: 0.189387\tvalid_1's rmse: 0.00045089\tvalid_1's RMSPE: 0.208405\n",
      "[5250]\ttraining's rmse: 0.000407297\ttraining's RMSPE: 0.18853\tvalid_1's rmse: 0.000450104\tvalid_1's RMSPE: 0.208041\n",
      "[5500]\ttraining's rmse: 0.000405397\ttraining's RMSPE: 0.18765\tvalid_1's rmse: 0.000449178\tvalid_1's RMSPE: 0.207613\n",
      "[5750]\ttraining's rmse: 0.000403575\ttraining's RMSPE: 0.186807\tvalid_1's rmse: 0.000448412\tvalid_1's RMSPE: 0.207259\n",
      "[6000]\ttraining's rmse: 0.00040188\ttraining's RMSPE: 0.186022\tvalid_1's rmse: 0.000447621\tvalid_1's RMSPE: 0.206893\n",
      "[6250]\ttraining's rmse: 0.000400174\ttraining's RMSPE: 0.185233\tvalid_1's rmse: 0.000446824\tvalid_1's RMSPE: 0.206525\n",
      "[6500]\ttraining's rmse: 0.00039856\ttraining's RMSPE: 0.184486\tvalid_1's rmse: 0.000446122\tvalid_1's RMSPE: 0.206201\n",
      "[6750]\ttraining's rmse: 0.000397003\ttraining's RMSPE: 0.183765\tvalid_1's rmse: 0.000445302\tvalid_1's RMSPE: 0.205822\n",
      "[7000]\ttraining's rmse: 0.00039559\ttraining's RMSPE: 0.183111\tvalid_1's rmse: 0.000444716\tvalid_1's RMSPE: 0.205551\n",
      "[7250]\ttraining's rmse: 0.000394183\ttraining's RMSPE: 0.182459\tvalid_1's rmse: 0.000444162\tvalid_1's RMSPE: 0.205295\n",
      "[7500]\ttraining's rmse: 0.000392829\ttraining's RMSPE: 0.181833\tvalid_1's rmse: 0.000443821\tvalid_1's RMSPE: 0.205137\n",
      "[7750]\ttraining's rmse: 0.000391503\ttraining's RMSPE: 0.181219\tvalid_1's rmse: 0.00044384\tvalid_1's RMSPE: 0.205146\n",
      "[8000]\ttraining's rmse: 0.000390211\ttraining's RMSPE: 0.180621\tvalid_1's rmse: 0.000443298\tvalid_1's RMSPE: 0.204895\n",
      "[8250]\ttraining's rmse: 0.000388931\ttraining's RMSPE: 0.180029\tvalid_1's rmse: 0.000442817\tvalid_1's RMSPE: 0.204673\n",
      "[8500]\ttraining's rmse: 0.000387596\ttraining's RMSPE: 0.17941\tvalid_1's rmse: 0.00044237\tvalid_1's RMSPE: 0.204466\n",
      "[8750]\ttraining's rmse: 0.00038635\ttraining's RMSPE: 0.178834\tvalid_1's rmse: 0.000441966\tvalid_1's RMSPE: 0.20428\n",
      "[9000]\ttraining's rmse: 0.00038504\ttraining's RMSPE: 0.178227\tvalid_1's rmse: 0.000441831\tvalid_1's RMSPE: 0.204217\n",
      "[9250]\ttraining's rmse: 0.000383847\ttraining's RMSPE: 0.177675\tvalid_1's rmse: 0.000441523\tvalid_1's RMSPE: 0.204075\n",
      "[9500]\ttraining's rmse: 0.0003827\ttraining's RMSPE: 0.177144\tvalid_1's rmse: 0.000441245\tvalid_1's RMSPE: 0.203947\n",
      "[9750]\ttraining's rmse: 0.000381454\ttraining's RMSPE: 0.176567\tvalid_1's rmse: 0.000440877\tvalid_1's RMSPE: 0.203777\n",
      "[10000]\ttraining's rmse: 0.00038024\ttraining's RMSPE: 0.176005\tvalid_1's rmse: 0.000440448\tvalid_1's RMSPE: 0.203578\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's rmse: 0.00038024\ttraining's RMSPE: 0.176005\tvalid_1's rmse: 0.000440448\tvalid_1's RMSPE: 0.203578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [1:16:50, 574.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved \n",
      "==================================\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[250]\ttraining's rmse: 0.000507508\ttraining's RMSPE: 0.234888\tvalid_1's rmse: 0.000516435\tvalid_1's RMSPE: 0.238946\n",
      "[500]\ttraining's rmse: 0.000478749\ttraining's RMSPE: 0.221578\tvalid_1's rmse: 0.000489091\tvalid_1's RMSPE: 0.226295\n",
      "[750]\ttraining's rmse: 0.00046773\ttraining's RMSPE: 0.216478\tvalid_1's rmse: 0.000480237\tvalid_1's RMSPE: 0.222198\n",
      "[1000]\ttraining's rmse: 0.000459696\ttraining's RMSPE: 0.21276\tvalid_1's rmse: 0.000474571\tvalid_1's RMSPE: 0.219577\n",
      "[1250]\ttraining's rmse: 0.000453536\ttraining's RMSPE: 0.209909\tvalid_1's rmse: 0.000470318\tvalid_1's RMSPE: 0.217609\n",
      "[1500]\ttraining's rmse: 0.000448089\ttraining's RMSPE: 0.207388\tvalid_1's rmse: 0.000466791\tvalid_1's RMSPE: 0.215977\n",
      "[1750]\ttraining's rmse: 0.000443522\ttraining's RMSPE: 0.205274\tvalid_1's rmse: 0.000463906\tvalid_1's RMSPE: 0.214642\n",
      "[2000]\ttraining's rmse: 0.000439473\ttraining's RMSPE: 0.2034\tvalid_1's rmse: 0.000461239\tvalid_1's RMSPE: 0.213408\n",
      "[2250]\ttraining's rmse: 0.000435944\ttraining's RMSPE: 0.201767\tvalid_1's rmse: 0.000459337\tvalid_1's RMSPE: 0.212528\n",
      "[2500]\ttraining's rmse: 0.000432807\ttraining's RMSPE: 0.200315\tvalid_1's rmse: 0.000457599\tvalid_1's RMSPE: 0.211724\n",
      "[2750]\ttraining's rmse: 0.000429644\ttraining's RMSPE: 0.198851\tvalid_1's rmse: 0.000455784\tvalid_1's RMSPE: 0.210884\n",
      "[3000]\ttraining's rmse: 0.000426827\ttraining's RMSPE: 0.197547\tvalid_1's rmse: 0.000454215\tvalid_1's RMSPE: 0.210158\n",
      "[3250]\ttraining's rmse: 0.000424467\ttraining's RMSPE: 0.196455\tvalid_1's rmse: 0.000453219\tvalid_1's RMSPE: 0.209697\n",
      "[3500]\ttraining's rmse: 0.000422102\ttraining's RMSPE: 0.19536\tvalid_1's rmse: 0.000452119\tvalid_1's RMSPE: 0.209188\n",
      "[3750]\ttraining's rmse: 0.000419821\ttraining's RMSPE: 0.194305\tvalid_1's rmse: 0.00045102\tvalid_1's RMSPE: 0.20868\n",
      "[4000]\ttraining's rmse: 0.000417629\ttraining's RMSPE: 0.19329\tvalid_1's rmse: 0.000450096\tvalid_1's RMSPE: 0.208252\n",
      "[4250]\ttraining's rmse: 0.000415389\ttraining's RMSPE: 0.192253\tvalid_1's rmse: 0.000448822\tvalid_1's RMSPE: 0.207663\n",
      "[4500]\ttraining's rmse: 0.000413438\ttraining's RMSPE: 0.19135\tvalid_1's rmse: 0.000447943\tvalid_1's RMSPE: 0.207256\n",
      "[4750]\ttraining's rmse: 0.000411433\ttraining's RMSPE: 0.190422\tvalid_1's rmse: 0.000447086\tvalid_1's RMSPE: 0.20686\n",
      "[5000]\ttraining's rmse: 0.000409628\ttraining's RMSPE: 0.189587\tvalid_1's rmse: 0.000446377\tvalid_1's RMSPE: 0.206532\n",
      "[5250]\ttraining's rmse: 0.000407763\ttraining's RMSPE: 0.188724\tvalid_1's rmse: 0.000445494\tvalid_1's RMSPE: 0.206123\n",
      "[5500]\ttraining's rmse: 0.000405864\ttraining's RMSPE: 0.187845\tvalid_1's rmse: 0.000444583\tvalid_1's RMSPE: 0.205702\n",
      "[5750]\ttraining's rmse: 0.000403967\ttraining's RMSPE: 0.186967\tvalid_1's rmse: 0.00044382\tvalid_1's RMSPE: 0.205348\n",
      "[6000]\ttraining's rmse: 0.000402317\ttraining's RMSPE: 0.186203\tvalid_1's rmse: 0.000443185\tvalid_1's RMSPE: 0.205055\n",
      "[6250]\ttraining's rmse: 0.000400451\ttraining's RMSPE: 0.18534\tvalid_1's rmse: 0.00044228\tvalid_1's RMSPE: 0.204636\n",
      "[6500]\ttraining's rmse: 0.000398853\ttraining's RMSPE: 0.1846\tvalid_1's rmse: 0.000441583\tvalid_1's RMSPE: 0.204313\n",
      "[6750]\ttraining's rmse: 0.000397238\ttraining's RMSPE: 0.183853\tvalid_1's rmse: 0.000441046\tvalid_1's RMSPE: 0.204065\n",
      "[7000]\ttraining's rmse: 0.000395713\ttraining's RMSPE: 0.183147\tvalid_1's rmse: 0.000440476\tvalid_1's RMSPE: 0.203801\n",
      "[7250]\ttraining's rmse: 0.000394373\ttraining's RMSPE: 0.182527\tvalid_1's rmse: 0.000440149\tvalid_1's RMSPE: 0.20365\n",
      "[7500]\ttraining's rmse: 0.0003929\ttraining's RMSPE: 0.181845\tvalid_1's rmse: 0.000439634\tvalid_1's RMSPE: 0.203412\n",
      "[7750]\ttraining's rmse: 0.000391503\ttraining's RMSPE: 0.181198\tvalid_1's rmse: 0.000439425\tvalid_1's RMSPE: 0.203315\n",
      "[8000]\ttraining's rmse: 0.000390043\ttraining's RMSPE: 0.180522\tvalid_1's rmse: 0.000438841\tvalid_1's RMSPE: 0.203045\n",
      "[8250]\ttraining's rmse: 0.000388696\ttraining's RMSPE: 0.179899\tvalid_1's rmse: 0.000438483\tvalid_1's RMSPE: 0.202879\n",
      "[8500]\ttraining's rmse: 0.00038742\ttraining's RMSPE: 0.179309\tvalid_1's rmse: 0.000438137\tvalid_1's RMSPE: 0.202719\n",
      "[8750]\ttraining's rmse: 0.000386092\ttraining's RMSPE: 0.178694\tvalid_1's rmse: 0.00043766\tvalid_1's RMSPE: 0.202499\n",
      "[9000]\ttraining's rmse: 0.000384785\ttraining's RMSPE: 0.178089\tvalid_1's rmse: 0.000437267\tvalid_1's RMSPE: 0.202316\n",
      "[9250]\ttraining's rmse: 0.000383498\ttraining's RMSPE: 0.177493\tvalid_1's rmse: 0.000436816\tvalid_1's RMSPE: 0.202108\n",
      "[9500]\ttraining's rmse: 0.000382367\ttraining's RMSPE: 0.17697\tvalid_1's rmse: 0.000436593\tvalid_1's RMSPE: 0.202005\n",
      "[9750]\ttraining's rmse: 0.000381141\ttraining's RMSPE: 0.176403\tvalid_1's rmse: 0.00043633\tvalid_1's RMSPE: 0.201883\n",
      "[10000]\ttraining's rmse: 0.000379989\ttraining's RMSPE: 0.175869\tvalid_1's rmse: 0.000436028\tvalid_1's RMSPE: 0.201743\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's rmse: 0.000379989\ttraining's RMSPE: 0.175869\tvalid_1's rmse: 0.000436028\tvalid_1's RMSPE: 0.201743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [1:26:32, 577.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved \n",
      "==================================\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[250]\ttraining's rmse: 0.000508227\ttraining's RMSPE: 0.235083\tvalid_1's rmse: 0.000534321\tvalid_1's RMSPE: 0.248524\n",
      "[500]\ttraining's rmse: 0.000479329\ttraining's RMSPE: 0.221716\tvalid_1's rmse: 0.000507865\tvalid_1's RMSPE: 0.236219\n",
      "[750]\ttraining's rmse: 0.000468595\ttraining's RMSPE: 0.216751\tvalid_1's rmse: 0.00049981\tvalid_1's RMSPE: 0.232472\n",
      "[1000]\ttraining's rmse: 0.000460589\ttraining's RMSPE: 0.213048\tvalid_1's rmse: 0.000495246\tvalid_1's RMSPE: 0.230349\n",
      "[1250]\ttraining's rmse: 0.000454304\ttraining's RMSPE: 0.210141\tvalid_1's rmse: 0.00049068\tvalid_1's RMSPE: 0.228226\n",
      "[1500]\ttraining's rmse: 0.000448974\ttraining's RMSPE: 0.207675\tvalid_1's rmse: 0.000487305\tvalid_1's RMSPE: 0.226656\n",
      "[1750]\ttraining's rmse: 0.00044438\ttraining's RMSPE: 0.205551\tvalid_1's rmse: 0.000484801\tvalid_1's RMSPE: 0.225491\n",
      "[2000]\ttraining's rmse: 0.000440355\ttraining's RMSPE: 0.203689\tvalid_1's rmse: 0.000482724\tvalid_1's RMSPE: 0.224525\n",
      "[2250]\ttraining's rmse: 0.000436816\ttraining's RMSPE: 0.202052\tvalid_1's rmse: 0.000481521\tvalid_1's RMSPE: 0.223966\n",
      "[2500]\ttraining's rmse: 0.000433647\ttraining's RMSPE: 0.200586\tvalid_1's rmse: 0.000480082\tvalid_1's RMSPE: 0.223296\n",
      "[2750]\ttraining's rmse: 0.000430563\ttraining's RMSPE: 0.199159\tvalid_1's rmse: 0.000479056\tvalid_1's RMSPE: 0.222819\n",
      "[3000]\ttraining's rmse: 0.000427912\ttraining's RMSPE: 0.197933\tvalid_1's rmse: 0.000477681\tvalid_1's RMSPE: 0.22218\n",
      "[3250]\ttraining's rmse: 0.00042533\ttraining's RMSPE: 0.196739\tvalid_1's rmse: 0.000476342\tvalid_1's RMSPE: 0.221557\n",
      "[3500]\ttraining's rmse: 0.000422985\ttraining's RMSPE: 0.195654\tvalid_1's rmse: 0.000475781\tvalid_1's RMSPE: 0.221296\n",
      "[3750]\ttraining's rmse: 0.00042069\ttraining's RMSPE: 0.194592\tvalid_1's rmse: 0.000475066\tvalid_1's RMSPE: 0.220963\n",
      "[4000]\ttraining's rmse: 0.000418278\ttraining's RMSPE: 0.193477\tvalid_1's rmse: 0.000473661\tvalid_1's RMSPE: 0.22031\n",
      "[4250]\ttraining's rmse: 0.000416127\ttraining's RMSPE: 0.192482\tvalid_1's rmse: 0.000473549\tvalid_1's RMSPE: 0.220258\n",
      "[4500]\ttraining's rmse: 0.000414029\ttraining's RMSPE: 0.191511\tvalid_1's rmse: 0.000472507\tvalid_1's RMSPE: 0.219773\n",
      "[4750]\ttraining's rmse: 0.000411998\ttraining's RMSPE: 0.190572\tvalid_1's rmse: 0.000471992\tvalid_1's RMSPE: 0.219534\n",
      "[5000]\ttraining's rmse: 0.000410006\ttraining's RMSPE: 0.18965\tvalid_1's rmse: 0.000471202\tvalid_1's RMSPE: 0.219166\n",
      "[5250]\ttraining's rmse: 0.000408198\ttraining's RMSPE: 0.188814\tvalid_1's rmse: 0.000470734\tvalid_1's RMSPE: 0.218948\n",
      "[5500]\ttraining's rmse: 0.000406569\ttraining's RMSPE: 0.188061\tvalid_1's rmse: 0.000470259\tvalid_1's RMSPE: 0.218727\n",
      "[5750]\ttraining's rmse: 0.00040461\ttraining's RMSPE: 0.187155\tvalid_1's rmse: 0.000469068\tvalid_1's RMSPE: 0.218173\n",
      "[6000]\ttraining's rmse: 0.000402864\ttraining's RMSPE: 0.186347\tvalid_1's rmse: 0.000468331\tvalid_1's RMSPE: 0.217831\n",
      "[6250]\ttraining's rmse: 0.000401151\ttraining's RMSPE: 0.185555\tvalid_1's rmse: 0.000467895\tvalid_1's RMSPE: 0.217628\n",
      "[6500]\ttraining's rmse: 0.000399548\ttraining's RMSPE: 0.184813\tvalid_1's rmse: 0.00046747\tvalid_1's RMSPE: 0.21743\n",
      "[6750]\ttraining's rmse: 0.000397976\ttraining's RMSPE: 0.184086\tvalid_1's rmse: 0.000466866\tvalid_1's RMSPE: 0.217149\n",
      "[7000]\ttraining's rmse: 0.000396487\ttraining's RMSPE: 0.183397\tvalid_1's rmse: 0.000466506\tvalid_1's RMSPE: 0.216982\n",
      "[7250]\ttraining's rmse: 0.000395095\ttraining's RMSPE: 0.182753\tvalid_1's rmse: 0.000466056\tvalid_1's RMSPE: 0.216773\n",
      "[7500]\ttraining's rmse: 0.00039368\ttraining's RMSPE: 0.182099\tvalid_1's rmse: 0.000465581\tvalid_1's RMSPE: 0.216552\n",
      "[7750]\ttraining's rmse: 0.000392246\ttraining's RMSPE: 0.181436\tvalid_1's rmse: 0.000464725\tvalid_1's RMSPE: 0.216154\n",
      "[8000]\ttraining's rmse: 0.000390918\ttraining's RMSPE: 0.180821\tvalid_1's rmse: 0.000464307\tvalid_1's RMSPE: 0.215959\n",
      "[8250]\ttraining's rmse: 0.000389469\ttraining's RMSPE: 0.180151\tvalid_1's rmse: 0.000463483\tvalid_1's RMSPE: 0.215576\n",
      "[8500]\ttraining's rmse: 0.000388106\ttraining's RMSPE: 0.179521\tvalid_1's rmse: 0.000462999\tvalid_1's RMSPE: 0.215351\n",
      "[8750]\ttraining's rmse: 0.000386795\ttraining's RMSPE: 0.178914\tvalid_1's rmse: 0.000462908\tvalid_1's RMSPE: 0.215309\n",
      "[9000]\ttraining's rmse: 0.000385506\ttraining's RMSPE: 0.178318\tvalid_1's rmse: 0.000462677\tvalid_1's RMSPE: 0.215201\n",
      "[9250]\ttraining's rmse: 0.000384291\ttraining's RMSPE: 0.177756\tvalid_1's rmse: 0.000462291\tvalid_1's RMSPE: 0.215021\n",
      "[9500]\ttraining's rmse: 0.000383069\ttraining's RMSPE: 0.177191\tvalid_1's rmse: 0.000461951\tvalid_1's RMSPE: 0.214863\n",
      "[9750]\ttraining's rmse: 0.000381765\ttraining's RMSPE: 0.176587\tvalid_1's rmse: 0.000461376\tvalid_1's RMSPE: 0.214596\n",
      "[10000]\ttraining's rmse: 0.000380619\ttraining's RMSPE: 0.176057\tvalid_1's rmse: 0.000461588\tvalid_1's RMSPE: 0.214694\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's rmse: 0.000380619\ttraining's RMSPE: 0.176057\tvalid_1's rmse: 0.000461588\tvalid_1's RMSPE: 0.214694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [1:36:43, 580.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved \n",
      "==================================\n",
      "0.2045578009097126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "oof_df = util.fit_model(params,X_train,y_train,features,cats=['stock_id'],n_fold=10,seed=66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['time_id', 'stock_id', 'book_wap1_lambda_', 'book_wap2_lambda_',\n",
       "       'book_wap_mean_lambda_', 'book_wap_diff_lambda_',\n",
       "       'book_price_spread_lambda_', 'book_bid_spread_lambda_',\n",
       "       'book_ask_spread_lambda_', 'book_total_volume_lambda_',\n",
       "       'book_volume_imbalance_lambda_', 'pricesum', 'pricemean', 'pricestd',\n",
       "       'pricemax', 'pricemin', 'sizesum', 'sizemean', 'sizestd', 'sizemax',\n",
       "       'sizemin', 'order_countsum', 'order_countmean', 'order_countstd',\n",
       "       'order_countmax', 'order_countmin', 'seconds_in_bucketsum',\n",
       "       'seconds_in_bucketmean', 'seconds_in_bucketstd', 'seconds_in_bucketmax',\n",
       "       'seconds_in_bucketmin', '__book_wap1_lambda_____mean___time',\n",
       "       '__book_wap1_lambda_____std___time',\n",
       "       '__book_wap1_lambda_____max___time',\n",
       "       '__book_wap1_lambda_____min___time',\n",
       "       '__book_wap2_lambda_____mean___time',\n",
       "       '__book_wap2_lambda_____std___time',\n",
       "       '__book_wap2_lambda_____max___time',\n",
       "       '__book_wap2_lambda_____min___time',\n",
       "       '__book_wap_mean_lambda_____mean___time',\n",
       "       '__book_wap_mean_lambda_____std___time',\n",
       "       '__book_wap_mean_lambda_____max___time',\n",
       "       '__book_wap_mean_lambda_____min___time',\n",
       "       '__book_wap_diff_lambda_____mean___time',\n",
       "       '__book_wap_diff_lambda_____std___time',\n",
       "       '__book_wap_diff_lambda_____max___time',\n",
       "       '__book_wap_diff_lambda_____min___time',\n",
       "       '__book_wap1_lambda_____mean___stock',\n",
       "       '__book_wap1_lambda_____std___stock',\n",
       "       '__book_wap1_lambda_____max___stock',\n",
       "       '__book_wap1_lambda_____min___stock',\n",
       "       '__book_wap2_lambda_____mean___stock',\n",
       "       '__book_wap2_lambda_____std___stock',\n",
       "       '__book_wap2_lambda_____max___stock',\n",
       "       '__book_wap2_lambda_____min___stock',\n",
       "       '__book_wap_mean_lambda_____mean___stock',\n",
       "       '__book_wap_mean_lambda_____std___stock',\n",
       "       '__book_wap_mean_lambda_____max___stock',\n",
       "       '__book_wap_mean_lambda_____min___stock',\n",
       "       '__book_wap_diff_lambda_____mean___stock',\n",
       "       '__book_wap_diff_lambda_____std___stock',\n",
       "       '__book_wap_diff_lambda_____max___stock',\n",
       "       '__book_wap_diff_lambda_____min___stock', 'target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
