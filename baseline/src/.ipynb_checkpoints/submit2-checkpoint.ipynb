{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from tqdm import tqdm \n",
    "from glob import glob\n",
    "import joblib\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import copy as cp\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '../input/optiver-realized-volatility-prediction'\n",
    "model_dir = '../input/quant-model'\n",
    "stock_encode_dir = '../input/quant-stock-encode'\n",
    "features_name_dir = '../input/quant-features-name'\n",
    "features_name_dir = '../data'\n",
    "stock_encode_dir = '../data'\n",
    "model_dir = '../model'\n",
    "input_dir = '../data'\n",
    "# input_dir = '../data'\n",
    "# model_dir = '../model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_book(stock_id=0, data_type='train'):\n",
    "    \"\"\"加载 book 数据\n",
    "    \"\"\"\n",
    "    book_df = pd.read_parquet(\n",
    "        os.path.join(input_dir,\n",
    "                     'book_{}.parquet/stock_id={}'.format(data_type,\n",
    "                                                          stock_id)))\n",
    "    book_df['stock_id'] = stock_id\n",
    "    book_df['stock_id'] = book_df['stock_id'].astype(np.int8)\n",
    "    book_df['seconds_in_bucket'] = book_df['seconds_in_bucket'].astype(\n",
    "        np.int32)\n",
    "\n",
    "    return book_df\n",
    "\n",
    "\n",
    "def load_trade(stock_id=0, data_type='train'):\n",
    "    \"\"\"加载 trade 数据\n",
    "    \"\"\"\n",
    "    trade_df = pd.read_parquet(\n",
    "        os.path.join(\n",
    "            input_dir,\n",
    "            'trade_{}.parquet/stock_id={}'.format(data_type, stock_id)))\n",
    "    trade_df['stock_id'] = stock_id\n",
    "    trade_df['stock_id'] = trade_df['stock_id'].astype(np.int8)\n",
    "    trade_df['order_count'] = trade_df['order_count'].astype(np.int32)\n",
    "    trade_df['seconds_in_bucket'] = trade_df['seconds_in_bucket'].astype(\n",
    "        np.int32)\n",
    "\n",
    "    return trade_df\n",
    "def log_return(list_stock_prices):\n",
    "    \"\"\"收益率\n",
    "    \"\"\"\n",
    "    return np.log(list_stock_prices).diff()\n",
    "\n",
    "\n",
    "def realized_volatility(series_log_return):\n",
    "    \"\"\"波动率\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.sum(series_log_return**2))\n",
    "\n",
    "\n",
    "def fix_jsonerr(df):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    df.columns = [\n",
    "        \"\".join(c if c.isalnum() else \"_\" for c in str(x)) for x in df.columns\n",
    "    ]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征工程\n",
    "def feature_row(book):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # book_wap1 生成标签\n",
    "    for i in [\n",
    "            1,\n",
    "            2,\n",
    "    ]:\n",
    "        # wap\n",
    "        book[f'book_wap{i}'] = (book[f'bid_price{i}'] * book[f'ask_size{i}'] +\n",
    "                                book[f'ask_price{i}'] *\n",
    "                                book[f'bid_size{i}']) / (book[f'bid_size{i}'] +\n",
    "                                                         book[f'ask_size{i}'])\n",
    "\n",
    "    # mean wap\n",
    "    book['book_wap_mean'] = (book['book_wap1'] + book['book_wap2']) / 2\n",
    "\n",
    "    # wap diff\n",
    "    book['book_wap_diff'] = book['book_wap1'] - book['book_wap2']\n",
    "\n",
    "    # other orderbook features\n",
    "    book['book_price_spread'] = (book['ask_price1'] - book['bid_price1']) / (\n",
    "        book['ask_price1'] + book['bid_price1'])\n",
    "    book['book_bid_spread'] = book['bid_price1'] - book['bid_price2']\n",
    "    book['book_ask_spread'] = book['ask_price1'] - book['ask_price2']\n",
    "    book['book_total_volume'] = book['ask_size1'] + book['ask_size2'] + book[\n",
    "        'bid_size1'] + book['bid_size2']\n",
    "    book['book_volume_imbalance'] = (book['ask_size1'] + book['ask_size2']) - (\n",
    "        book['bid_size1'] + book['bid_size2'])\n",
    "    return book\n",
    "\n",
    "\n",
    "def feature_agg(book, trade):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # 聚合生成特征\n",
    "    book_feats = book.columns[book.columns.str.startswith('book_')].tolist()\n",
    "    trade_feats = ['price', 'size', 'order_count', 'seconds_in_bucket']\n",
    "\n",
    "    trade = trade.groupby(['time_id', 'stock_id'])[trade_feats].agg(\n",
    "        ['sum', 'mean', 'std', 'max', 'min']).reset_index()\n",
    "\n",
    "    book = book.groupby(['time_id', 'stock_id'])[book_feats].agg(\n",
    "        [lambda x: realized_volatility(log_return(x))]).reset_index()\n",
    "\n",
    "    # 修改特征名称\n",
    "    book.columns = [\"\".join(col).strip() for col in book.columns.values]\n",
    "    trade.columns = [\"\".join(col).strip() for col in trade.columns.values]\n",
    "    df_ret = book.merge(trade, how='left', on=['time_id', 'stock_id'])\n",
    "    return df_ret\n",
    "\n",
    "\n",
    "def gen_data_train(stock_id=0):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    book = load_book(stock_id, 'train')\n",
    "    trade = load_trade(stock_id, 'train')\n",
    "\n",
    "    book = book.sort_values(by=['time_id', 'seconds_in_bucket'])\n",
    "    trade = trade.sort_values(by=['time_id', 'seconds_in_bucket'])\n",
    "\n",
    "    book = feature_row(book)\n",
    "\n",
    "    df_ret1 = feature_agg(book, trade)\n",
    "\n",
    "    df_ret2 = calculate_features2(book, trade)\n",
    "\n",
    "    return df_ret2.merge(df_ret1, how='left', on='time_id')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def gen_data_test(stock_id=0):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    book = load_book(stock_id, 'test')\n",
    "    trade = load_trade(stock_id, 'test')\n",
    "\n",
    "    book = book.sort_values(by=['time_id', 'seconds_in_bucket'])\n",
    "    trade = trade.sort_values(by=['time_id', 'seconds_in_bucket'])\n",
    "\n",
    "    book = feature_row(book)\n",
    "\n",
    "    df_ret1 = feature_agg(book, trade)\n",
    "    print(df_ret1)\n",
    "    df_ret2 = calculate_features2(book, trade)\n",
    "    print(df_ret2)\n",
    "    return df_ret2.merge(df_ret1, how='left', on='time_id')\n",
    "\n",
    "\n",
    "def gen_data_multi(stock_lst, data_type='train'):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    with Pool(cpu_count()) as p:\n",
    "        if data_type == 'train':\n",
    "            feature_dfs = list(\n",
    "                tqdm(p.imap(gen_data_train, stock_lst), total=len(stock_lst)))\n",
    "        if data_type == 'test':\n",
    "            feature_dfs = list(\n",
    "                tqdm(p.imap(gen_data_test, stock_lst), total=len(stock_lst)))\n",
    "    print(feature_dfs)\n",
    "    df_ret = pd.concat(feature_dfs)\n",
    "    return df_ret\n",
    "\n",
    "\n",
    "def gen_data_encoding(df_ret, df_label, data_type='train'):\n",
    "    \"\"\"\n",
    "    test 不使用自己数据的 stock_id encoding\n",
    "    \"\"\"\n",
    "\n",
    "    # 对 stock_id 进行 encoding\n",
    "    vol_feats = [f for f in df_ret.columns if ('lambda' in f) & ('wap' in f)]\n",
    "    if data_type == 'train':\n",
    "        # agg\n",
    "        stock_df = df_ret.groupby('stock_id')[vol_feats].agg([\n",
    "            'mean',\n",
    "            'std',\n",
    "            'max',\n",
    "            'min',\n",
    "        ]).reset_index()\n",
    "\n",
    "        # fix column names\n",
    "        stock_df.columns = ['stock_id'] + [\n",
    "            f'{f}_stock' for f in stock_df.columns.values.tolist()[1:]\n",
    "        ]\n",
    "        stock_df = fix_jsonerr(stock_df)\n",
    "\n",
    "    # 对 time_id 进行 encoding\n",
    "    time_df = df_ret.groupby('time_id')[vol_feats].agg([\n",
    "        'mean',\n",
    "        'std',\n",
    "        'max',\n",
    "        'min',\n",
    "    ]).reset_index()\n",
    "    time_df.columns = ['time_id'] + [\n",
    "        f'{f}_time' for f in time_df.columns.values.tolist()[1:]\n",
    "    ]\n",
    "\n",
    "    # merge\n",
    "    df_ret = df_ret.merge(time_df, how='left', on='time_id')\n",
    "\n",
    "    # make sure to fix json error for lighgbm\n",
    "    df_ret = fix_jsonerr(df_ret)\n",
    "\n",
    "    # out\n",
    "    if data_type == 'train':\n",
    "        df_ret = df_ret.merge(stock_df, how='left', on='stock_id').merge(\n",
    "            df_label, how='left',\n",
    "            on=['stock_id', 'time_id']).replace([np.inf, -np.inf],\n",
    "                                                np.nan).fillna(method='ffill')\n",
    "        return df_ret\n",
    "    if data_type == 'test':\n",
    "        stock_df = pd.read_pickle(os.path.join(stock_encode_dir,'20210809.pkl'))\n",
    "        df_ret = df_ret.merge(stock_df, how='left', on='stock_id').replace(\n",
    "            [np.inf, -np.inf], np.nan).fillna(method='ffill')\n",
    "        return df_ret\n",
    "\n",
    "\n",
    "def calc_rollingstats(rolling_x, roll_name):\n",
    "    #统计量\n",
    "    if len(rolling_x) > 0:\n",
    "        roll_autocorr = rolling_x.groupby(\"time_id\")[[roll_name,\n",
    "                                                      \"xpre\"]].corr()\n",
    "        roll_autocorr.reset_index(inplace=True)\n",
    "        roll_autocorr = roll_autocorr.groupby(\"time_id\").head(1)\n",
    "        roll_autocorr.index = roll_autocorr[\"time_id\"]\n",
    "        del roll_autocorr[\"time_id\"]\n",
    "\n",
    "        roll_autocorr = pd.DataFrame(\n",
    "            {roll_name + \"_autocorr\": roll_autocorr[\"xpre\"]})\n",
    "\n",
    "        roll_mean = pd.DataFrame({\n",
    "            roll_name + \"_mean\":\n",
    "            rolling_x.groupby(\"time_id\")[roll_name].mean()\n",
    "        })\n",
    "        roll_std = pd.DataFrame({\n",
    "            roll_name + \"_std\":\n",
    "            rolling_x.groupby(\"time_id\")[roll_name].std()\n",
    "        })\n",
    "        roll_skew = pd.DataFrame({\n",
    "            roll_name + \"_skew\":\n",
    "            rolling_x.groupby(\"time_id\")[roll_name].skew()\n",
    "        })\n",
    "\n",
    "        data_merge = pd.merge(roll_mean,\n",
    "                              roll_std,\n",
    "                              left_index=True,\n",
    "                              right_index=True,\n",
    "                              how=\"inner\")\n",
    "        data_merge = pd.merge(data_merge,\n",
    "                              roll_skew,\n",
    "                              left_index=True,\n",
    "                              right_index=True,\n",
    "                              how=\"inner\")\n",
    "        data_merge = pd.merge(data_merge,\n",
    "                              roll_autocorr,\n",
    "                              left_index=True,\n",
    "                              right_index=True,\n",
    "                              how=\"inner\")\n",
    "\n",
    "    else:\n",
    "\n",
    "        data_merge = pd.DataFrame([[np.nan, np.nan, np.nan, np.nan]])\n",
    "        data_merge.columns = [\n",
    "            roll_name + \"_mean\", roll_name + \"_std\", roll_name + \"_skew\",\n",
    "            roll_name + \"_autocorr\"\n",
    "        ]\n",
    "\n",
    "    return data_merge\n",
    "\n",
    "\n",
    "\n",
    "def make_candle(df_data, price_name, vol_name, amt_name):\n",
    "\n",
    "    df_data[\"pre\"] = df_data.groupby(\"time_id\")[price_name].shift(1)\n",
    "    df_data[\"ret\"] = df_data[price_name] / df_data[\"pre\"] - 1\n",
    "    df_data[\"absret\"] = abs(df_data[\"ret\"])\n",
    "    df_retsum = pd.DataFrame(\n",
    "        {\"retsum\": df_data.groupby(\"time_id\")[\"ret\"].sum()})\n",
    "    df_absretsum = pd.DataFrame(\n",
    "        {\"absretsum\": df_data.groupby(\"time_id\")[\"absret\"].sum()})\n",
    "\n",
    "    df_data[\"absobv\"] = df_data[\"absret\"] * df_data[vol_name]\n",
    "    df_obvabs = pd.DataFrame(\n",
    "        {\"xf4_abs\": df_data.groupby(\"time_id\")[\"absobv\"].sum()})\n",
    "\n",
    "    df_data[\"obv\"] = df_data[\"ret\"] * df_data[vol_name]\n",
    "    df_obv = pd.DataFrame({\"xf4\": df_data.groupby(\"time_id\")[\"obv\"].sum()})\n",
    "\n",
    "    df_amt = pd.DataFrame(\n",
    "        {amt_name + \"sum\": df_data.groupby(\"time_id\")[amt_name].sum()})\n",
    "    df_vol = pd.DataFrame(\n",
    "        {vol_name + \"sum\": df_data.groupby(\"time_id\")[vol_name].sum()})\n",
    "\n",
    "    df_mean = pd.DataFrame(\n",
    "        {price_name + \"mean\": df_data.groupby(\"time_id\")[price_name].mean()})\n",
    "    df_high = pd.DataFrame(\n",
    "        {price_name + \"high\": df_data.groupby(\"time_id\")[price_name].max()})\n",
    "    df_low = pd.DataFrame(\n",
    "        {price_name + \"low\": df_data.groupby(\"time_id\")[price_name].min()})\n",
    "\n",
    "    df_open = df_data.groupby(\"time_id\").head(1)\n",
    "    df_open.index = df_open[\"time_id\"]\n",
    "    df_open = pd.DataFrame({price_name + \"open\": df_open[price_name]})\n",
    "\n",
    "    df_close = df_data.groupby(\"time_id\").tail(1)\n",
    "    df_close.index = df_close[\"time_id\"]\n",
    "    df_close = pd.DataFrame({price_name + \"close\": df_close[price_name]})\n",
    "\n",
    "    df_candle = pd.merge(df_high,\n",
    "                         df_low,\n",
    "                         left_index=True,\n",
    "                         right_index=True,\n",
    "                         how=\"inner\")\n",
    "    df_candle = pd.merge(df_candle,\n",
    "                         df_mean,\n",
    "                         left_index=True,\n",
    "                         right_index=True,\n",
    "                         how=\"inner\")\n",
    "    df_candle = pd.merge(df_candle,\n",
    "                         df_open,\n",
    "                         left_index=True,\n",
    "                         right_index=True,\n",
    "                         how=\"inner\")\n",
    "    df_candle = pd.merge(df_candle,\n",
    "                         df_close,\n",
    "                         left_index=True,\n",
    "                         right_index=True,\n",
    "                         how=\"inner\")\n",
    "    df_candle = pd.merge(df_candle,\n",
    "                         df_vol,\n",
    "                         left_index=True,\n",
    "                         right_index=True,\n",
    "                         how=\"inner\")\n",
    "    df_candle = pd.merge(df_candle,\n",
    "                         df_amt,\n",
    "                         left_index=True,\n",
    "                         right_index=True,\n",
    "                         how=\"inner\")\n",
    "    df_candle = pd.merge(df_candle,\n",
    "                         df_retsum,\n",
    "                         left_index=True,\n",
    "                         right_index=True,\n",
    "                         how=\"inner\")\n",
    "    df_candle = pd.merge(df_candle,\n",
    "                         df_absretsum,\n",
    "                         left_index=True,\n",
    "                         right_index=True,\n",
    "                         how=\"inner\")\n",
    "    df_candle = pd.merge(df_candle,\n",
    "                         df_obvabs,\n",
    "                         left_index=True,\n",
    "                         right_index=True,\n",
    "                         how=\"inner\")\n",
    "    df_candle = pd.merge(df_candle,\n",
    "                         df_obv,\n",
    "                         left_index=True,\n",
    "                         right_index=True,\n",
    "                         how=\"inner\")\n",
    "\n",
    "    return df_candle\n",
    "\n",
    "\n",
    "def cal_candlefactor(df_candle, price_name, vol_name, amt_name):\n",
    "    f_name = price_name + \"candle\"\n",
    "    #f1:illiq\n",
    "    df_candle[f_name + \"f1\"] = (\n",
    "        2 * (df_candle[price_name + \"high\"] - df_candle[price_name + \"low\"]) -\n",
    "        abs(df_candle[price_name + \"open\"] -\n",
    "            df_candle[price_name + \"close\"])) / df_candle[amt_name + \"sum\"]\n",
    "    #f2 strength\n",
    "    df_candle[f_name + \"f2\"] = df_candle[\"retsum\"] / df_candle[\"absretsum\"]\n",
    "    #f3:ad\n",
    "    df_candle[f_name + \"f3\"] =  (2 *df_candle[price_name + \"close\"] - df_candle[price_name + \"low\"]\\\n",
    "                    - df_candle[price_name + \"high\"] )/(df_candle[price_name + \"high\"] - df_candle[price_name + \"low\"]) \\\n",
    "                    * df_candle[vol_name + \"sum\"]\n",
    "    #f3: obv\n",
    "    df_candle[f_name + \"f41\"] = df_candle[\"xf4\"] / df_candle[vol_name + \"sum\"]\n",
    "    df_candle[f_name +\n",
    "              \"f42\"] = df_candle[\"xf4_abs\"] / df_candle[vol_name + \"sum\"]\n",
    "    return df_candle\n",
    "\n",
    "\n",
    "def calculate_features2(book_df, trade_df):\n",
    "    \"\"\"\n",
    "    df: book_train data for each stock_id\n",
    "    \"\"\"\n",
    "    #calculate price for features\n",
    "\n",
    "    book_df['wap'] = (book_df['bid_price1'] * book_df['ask_size1'] +\n",
    "                      book_df['ask_price1'] * book_df['bid_size1']) / (\n",
    "                          book_df['bid_size1'] + book_df['ask_size1'])\n",
    "\n",
    "    book_df[\"vol_ab\"] = book_df['bid_size1'] + book_df['ask_size1']\n",
    "    book_df[\"amt_ab\"] = book_df['bid_price1'] * book_df['ask_size1'] + book_df[\n",
    "        'ask_price1'] * book_df['bid_size1']\n",
    "\n",
    "    book_df[\"amt_a\"] = book_df['ask_price1'] * book_df['ask_size1']\n",
    "    book_df[\"amt_b\"] = book_df['bid_price1'] * book_df['bid_size1']\n",
    "\n",
    "    trade_df[\"amt\"] = trade_df[\"price\"] * trade_df[\"size\"]\n",
    "\n",
    "    #flag filter\n",
    "    book_df[\"wap_pre\"] = book_df.groupby(\"time_id\")['wap'].shift(1)\n",
    "    book_df[\"bid_ppre\"] = book_df.groupby(\"time_id\")['bid_price1'].shift(1)\n",
    "    book_df[\"ask_ppre\"] = book_df.groupby(\"time_id\")['ask_price1'].shift(1)\n",
    "\n",
    "    book_df[\"isBS\"] = np.where(\n",
    "        book_df[\"wap\"] > book_df[\"wap_pre\"], \"B\",\n",
    "        np.where(book_df[\"wap\"] < book_df[\"wap_pre\"], \"S\", np.nan))\n",
    "    book_df[\"isBS_big\"] = np.where(\n",
    "        book_df[\"wap\"] > book_df[\"ask_ppre\"], \"supB\",\n",
    "        np.where(book_df[\"wap\"] < book_df[\"bid_ppre\"], \"supS\",\n",
    "                 np.where(pd.notnull(book_df[\"wap\"]), \"midBS\", np.nan)))\n",
    "\n",
    "    ordersize50 = pd.DataFrame({\n",
    "        \"ordersize50\":\n",
    "        book_df.groupby(\"time_id\")[\"amt_ab\"].apply(lambda x: np.nanmedian(x))\n",
    "    })\n",
    "    ordersize50.reset_index(inplace=True)\n",
    "\n",
    "    ordersize25 = pd.DataFrame({\n",
    "        \"ordersize25\":\n",
    "        book_df.groupby(\"time_id\")[\"amt_ab\"].apply(\n",
    "            lambda x: np.nanpercentile(x, 75))\n",
    "    })\n",
    "    ordersize25.reset_index(inplace=True)\n",
    "\n",
    "    ordersize75 = pd.DataFrame({\n",
    "        \"ordersize75\":\n",
    "        book_df.groupby(\"time_id\")[\"amt_ab\"].apply(\n",
    "            lambda x: np.nanpercentile(x, 25))\n",
    "    })\n",
    "    ordersize75.reset_index(inplace=True)\n",
    "    book_df1 = pd.merge(book_df, ordersize50, on=\"time_id\", how=\"left\")\n",
    "    book_df1 = pd.merge(book_df1, ordersize25, on=\"time_id\", how=\"left\")\n",
    "    book_df1 = pd.merge(book_df1, ordersize75, on=\"time_id\", how=\"left\")\n",
    "    book_df1.loc[:, \"isoversize50\"] = np.where(\n",
    "        book_df1[\"amt_ab\"] > book_df1[\"ordersize50\"], \"up50\",\n",
    "        np.where(book_df1[\"amt_ab\"] <= book_df1[\"ordersize50\"], \"down50\",\n",
    "                 np.nan))\n",
    "\n",
    "    book_df1.loc[:, \"isoversize75\"] = np.where(\n",
    "        book_df1[\"amt_ab\"] > book_df1[\"ordersize75\"], \"up75\",\n",
    "        np.where(book_df1[\"amt_ab\"] <= book_df1[\"ordersize75\"], \"down75\",\n",
    "                 np.nan))\n",
    "    book_df1.loc[:, \"isoversize25\"] = np.where(\n",
    "        book_df1[\"amt_ab\"] > book_df1[\"ordersize25\"], \"up25\",\n",
    "        np.where(book_df1[\"amt_ab\"] <= book_df1[\"ordersize25\"], \"down25\",\n",
    "                 np.nan))\n",
    "\n",
    "    #不同波动率\n",
    "    #calculate historical volatility\n",
    "    vol = book_df1.groupby('time_id')['wap'].apply(\n",
    "        lambda x: np.sqrt(np.sum(np.log(x).diff()**2)))\n",
    "    vol_df = pd.DataFrame(vol)\n",
    "    vol_df.rename(columns={'wap': 'vol_orig'}, inplace=True)\n",
    "    data_merge_all = vol_df\n",
    "\n",
    "    #修改波动率：\n",
    "    #    rolling波动率均值，标准差，偏度，自相关\n",
    "    #新指标,新指标均值，标准差，偏度，自相关\n",
    "\n",
    "    #    roll_name0 = \"roll_std\"\n",
    "    #    roll_window = 10\n",
    "    #BS FLAG\n",
    "    flagname = \"B\"\n",
    "    filtername = \"isBS\"\n",
    "    for filtername, flagname in [[\"isBS\", \"B\"], [\"isBS\", \"S\"],\n",
    "                                 [\"isBS_big\", \"supB\"], [\"isBS_big\", \"supS\"],\n",
    "                                 [\"isBS_big\", \"midBS\"],\n",
    "                                 [\"isoversize50\", \"up50\"],\n",
    "                                 [\"isoversize50\", \"down50\"],\n",
    "                                 [\"isoversize25\", \"up25\"],\n",
    "                                 [\"isoversize25\", \"down25\"],\n",
    "                                 [\"isoversize75\", \"up75\"],\n",
    "                                 [\"isoversize75\", \"down75\"]]:\n",
    "        print(filtername, flagname)\n",
    "        book_df_new = book_df1[book_df1[filtername] == flagname]\n",
    "        #个数\n",
    "        df_fnum = pd.DataFrame({\n",
    "            flagname + \"num\":\n",
    "            book_df_new.groupby(\"time_id\")[\"seconds_in_bucket\"].count()\n",
    "        })\n",
    "        data_merge_all = pd.merge(data_merge_all,\n",
    "                                  df_fnum,\n",
    "                                  left_index=True,\n",
    "                                  right_index=True,\n",
    "                                  how=\"left\")\n",
    "\n",
    "        for roll_window in [5, 10]:\n",
    "            #rolling指标\n",
    "            price_name = \"wap\"\n",
    "            roll_name0 = price_name + \"roll_std\"\n",
    "            roll_name = roll_name0 + str(roll_window) + \"_\" + flagname\n",
    "\n",
    "            rolling_x = pd.DataFrame({\n",
    "                roll_name:\n",
    "                book_df_new.groupby(\"time_id\")[price_name].rolling(\n",
    "                    roll_window).std()\n",
    "            })\n",
    "            rolling_x.reset_index(inplace=True)\n",
    "            rolling_x.loc[:, \"xpre\"] = rolling_x.groupby(\n",
    "                \"time_id\")[roll_name].shift(1)\n",
    "            #计算统计量因子\n",
    "            data_merge = calc_rollingstats(rolling_x, roll_name)\n",
    "            data_merge_all = pd.merge(data_merge_all,\n",
    "                                      data_merge,\n",
    "                                      left_index=True,\n",
    "                                      right_index=True,\n",
    "                                      how=\"left\")\n",
    "\n",
    "        #全局做candle：wap，买盘，卖盘\n",
    "        #candle因子\n",
    "        price_name = \"wap\"\n",
    "        vol_name = \"vol_ab\"\n",
    "        amt_name = \"amt_ab\"\n",
    "        df_data = cp.deepcopy(book_df_new)\n",
    "        df_candle = make_candle(df_data, price_name, vol_name, amt_name)\n",
    "\n",
    "        list_save = [\n",
    "            price_name + \"candlef1\", price_name + \"candlef2\",\n",
    "            price_name + \"candlef3\", price_name + \"candlef41\",\n",
    "            price_name + \"candlef42\"\n",
    "        ]\n",
    "\n",
    "        list_save = [i + \"_\" + flagname for i in list_save]\n",
    "        df_candle = cal_candlefactor(df_candle, price_name, vol_name, amt_name)\n",
    "\n",
    "        col_orig = list(df_candle.columns)\n",
    "        col_new = [i + \"_\" + flagname for i in col_orig]\n",
    "        df_candle.columns = col_new\n",
    "\n",
    "        data_merge_all = pd.merge(data_merge_all,\n",
    "                                  df_candle[list_save],\n",
    "                                  left_index=True,\n",
    "                                  right_index=True,\n",
    "                                  how=\"left\")\n",
    "\n",
    "    #加filter做candle\n",
    "    #    切割，打flag，给权重， 加filter切历史，波动率，分买入卖出，大单小单，上行下行，主买主卖\n",
    "    #    全天上行波动率/全天波动率\n",
    "    #index 数据   #复杂\n",
    "    #calculate max and min bid-ask spread\n",
    "    del data_merge_all[\"vol_orig\"]\n",
    "\n",
    "    return data_merge_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_lst = glob(os.path.join(input_dir,'book_test.parquet/*'))\n",
    "stock_lst = [os.path.basename(path).split('=')[-1] for path in path_lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name = pd.read_pickle(os.path.join(features_name_dir,'features_name_new.pkl')).values.reshape(-1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-47d934951bdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgen_data_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-49-4a2fc225f060>\u001b[0m in \u001b[0;36mgen_data_test\u001b[0;34m(stock_id)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \"\"\"\n\u001b[1;32m     76\u001b[0m     \"\"\"\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_book\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstock_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0mtrade\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_trade\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstock_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-13224c866bbb>\u001b[0m in \u001b[0;36mload_book\u001b[0;34m(stock_id, data_type)\u001b[0m\n\u001b[1;32m      5\u001b[0m         os.path.join(input_dir,\n\u001b[1;32m      6\u001b[0m                      'book_{}.parquet/stock_id={}'.format(data_type,\n\u001b[0;32m----> 7\u001b[0;31m                                                           stock_id)))\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mbook_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stock_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstock_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mbook_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stock_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbook_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stock_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/python36/lib/python3.6/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, **kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m     \"\"\"\n\u001b[1;32m    316\u001b[0m     \u001b[0mimpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/envs/python36/lib/python3.6/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, path, columns, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_filepath_or_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0mparquet_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParquetFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mparquet_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/python36/lib/python3.6/site-packages/fastparquet/api.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fn, verify, open_with, root, sep, fs)\u001b[0m\n\u001b[1;32m    147\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_attrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen_with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gen_data_test(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-2101ca600164>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_ret_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_data_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstock_lst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-49-4a2fc225f060>\u001b[0m in \u001b[0;36mgen_data_multi\u001b[0;34m(stock_lst, data_type)\u001b[0m\n\u001b[1;32m    101\u001b[0m                 tqdm(p.imap(gen_data_test, stock_lst), total=len(stock_lst)))\n\u001b[1;32m    102\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_dfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0mdf_ret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_dfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf_ret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/python36/lib/python3.6/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m     )\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/python36/lib/python3.6/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No objects to concatenate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "df_ret_test = gen_data_multi(stock_lst, data_type='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df_ret_test = gen_data_multi(stock_lst, data_type='test')\n",
    "except:\n",
    "    df_ret_test = pd.DataFrame(dict(zip(feature_name,[[0]]*len(feature_name))))\n",
    "    df_ret_test['stock_id'] = 0\n",
    "    df_ret_test['time_id'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_test = gen_data_encoding(df_ret_test, None, data_type = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['__book_wap_mean_lambda_____std___stock', '__book_wap_mean_lambda_____max___stock', '__book_wap1_lambda_____max___stock', '__book_wap_diff_lambda_____min___stock', '__book_wap2_lambda_____std___stock', '__book_wap1_lambda_____std___stock', '__book_wap_mean_lambda_____mean___stock', '__book_wap_diff_lambda_____std___stock', '__book_wap_diff_lambda_____max___stock', '__book_wap1_lambda_____min___stock', '__book_wap2_lambda_____mean___stock', '__book_wap_diff_lambda_____mean___stock', '__book_wap1_lambda_____mean___stock', '__book_wap2_lambda_____min___stock', '__book_wap2_lambda_____max___stock', '__book_wap_mean_lambda_____min___stock'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-b808380d557b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_lst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0my_preds\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_all_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0my_preds\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/python36/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2910\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2911\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2912\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2914\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/python36/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1252\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1255\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/python36/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1302\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m             \u001b[0;31m# we skip the warning on Categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['__book_wap_mean_lambda_____std___stock', '__book_wap_mean_lambda_____max___stock', '__book_wap1_lambda_____max___stock', '__book_wap_diff_lambda_____min___stock', '__book_wap2_lambda_____std___stock', '__book_wap1_lambda_____std___stock', '__book_wap_mean_lambda_____mean___stock', '__book_wap_diff_lambda_____std___stock', '__book_wap_diff_lambda_____max___stock', '__book_wap1_lambda_____min___stock', '__book_wap2_lambda_____mean___stock', '__book_wap_diff_lambda_____mean___stock', '__book_wap1_lambda_____mean___stock', '__book_wap2_lambda_____min___stock', '__book_wap2_lambda_____max___stock', '__book_wap_mean_lambda_____min___stock'] not in index\""
     ]
    }
   ],
   "source": [
    "y_preds = np.zeros(len(df_all_test))\n",
    "model_lst = glob(os.path.join(model_dir,'*model*.pkl'))\n",
    "for i, model_path in tqdm(enumerate(model_lst)):\n",
    "    model = joblib.load(model_path)\n",
    "    y_preds += model.predict(df_all_test[feature_name])\n",
    "y_preds /= (i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_test['row_id'] = df_all_test['stock_id'].astype(str)+'-'+df_all_test['time_id'].astype(str)\n",
    "df_all_test['target'] = y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = df_all_test[['row_id','target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('submission.csv',index = False)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
